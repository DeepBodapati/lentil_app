{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import listdir, makedirs, walk\n",
    "from os.path import isdir, isfile, join, exists, expanduser\n",
    "from tqdm import tqdm_notebook\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View underlying data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: 21, Training samples: 947, Validation samples: 341\n"
     ]
    }
   ],
   "source": [
    "train_folder      = 'data/train/'\n",
    "valid_folder      = 'data/validation/'\n",
    "weights_folder    = 'models/'\n",
    "\n",
    "bottleneck_features_train_npy = 'bottleneck_features_train.npy'\n",
    "bottleneck_features_valid_npy = 'bottleneck_features_valid.npy'\n",
    "bottleneck_labels_train_npy = 'bottleneck_labels_train.npy'\n",
    "bottleneck_labels_valid_npy = 'bottleneck_labels_valid.npy'\n",
    "\n",
    "top_model_epochs = 50\n",
    "complete_model_epochs = 50\n",
    "\n",
    "augmented_images_multiply_factor = 3\n",
    "\n",
    "# Create weights folder to save model weights\n",
    "if not exists(weights_folder):\n",
    "    makedirs(weights_folder)\n",
    "    \n",
    "ext = '.jpg'\n",
    "classes = [fldr for fldr in listdir(train_folder) if isdir(join(train_folder, fldr))]\n",
    "num_classes = len(classes)\n",
    "train_samples = sum([len(files) for r, d, files in walk(train_folder)])\n",
    "valid_samples = sum([len(files) for r, d, files in walk(valid_folder)])\n",
    "\n",
    "print (\"Classes: {}, Training samples: {}, Validation samples: {}\".format(num_classes, train_samples, valid_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "%matplotlib inline\n",
    "\n",
    "# options\n",
    "INPUT_SIZE = 299\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "fig = plt.figure(1, figsize=(16, 16))\n",
    "grid = ImageGrid(fig, 111, nrows_ncols=(3, 3), axes_pad=0.05)\n",
    "\n",
    "for i, cls in enumerate(np.random.choice(classes, 9)):\n",
    "    ax = grid[i]\n",
    "    img_file = np.random.choice(listdir(join(train_folder, cls)))\n",
    "    img = image.load_img(join(train_folder, cls, img_file), target_size=(INPUT_SIZE,INPUT_SIZE))\n",
    "    img = image.img_to_array(img)\n",
    "    ax.imshow(img / 255.)\n",
    "    ax.text(10, 200, 'LABEL: %s' % cls, color='k', backgroundcolor='w', alpha=0.8)\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapting [Keras example](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) for Xception model with multiclass classification\n",
    "\n",
    "1. Create base model and save bottleneck features\n",
    "2. Train top-model with bottleneck features\n",
    "3. Setup data to flow from directory with augmentation\n",
    "4. Fine tune base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.layers import Dropout, Flatten, Dense, Input, GlobalAveragePooling2D, BatchNormalization, deserialize\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications import xception\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create base model and save bottleneck features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating wrapper for input preprocessing from https://nbviewer.jupyter.org/gist/embanner/6149bba89c174af3bfd69537b72bca74 \n",
    "def preprocess_input_xception(x):\n",
    "    \"\"\"Wrapper around keras.applications.xception.preprocess_input()\n",
    "    to make it compatible for use with keras.preprocessing.image.ImageDataGenerator's\n",
    "    `preprocessing_function` argument.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : a numpy 3darray (a single image to be preprocessed)\n",
    "    \n",
    "    Note we cannot pass keras.applications.xception.preprocess_input()\n",
    "    directly to to keras.preprocessing.image.ImageDataGenerator's\n",
    "    `preprocessing_function` argument because the former expects a\n",
    "    4D tensor whereas the latter expects a 3D tensor. Hence the\n",
    "    existence of this wrapper.\n",
    "    \n",
    "    Returns a numpy 3darray (the preprocessed image).\n",
    "    \n",
    "    \"\"\"\n",
    "    from keras.applications.xception import preprocess_input\n",
    "    X = np.expand_dims(x, axis=0)\n",
    "    X = preprocess_input(X)\n",
    "    return X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = xception.Xception(weights='imagenet', include_top=False, input_tensor=Input(shape=(INPUT_SIZE,INPUT_SIZE,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 947 images belonging to 21 classes.\n",
      "60/60 [==============================] - 12s 203ms/step\n"
     ]
    }
   ],
   "source": [
    "if not exists(bottleneck_features_train_npy):\n",
    "    # Set up data generator for bottleneck features - only need preprocessing (no augmentation)\n",
    "    datagen = ImageDataGenerator(preprocessing_function=preprocess_input_xception)\n",
    "\n",
    "    # Generate bottleneck features for training data, and accompanying labels\n",
    "    generator = datagen.flow_from_directory(\n",
    "        train_folder,\n",
    "        target_size=(INPUT_SIZE, INPUT_SIZE),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='sparse',\n",
    "        shuffle=False)\n",
    "\n",
    "    bottleneck_features_train = base_model.predict_generator(generator, verbose = 1)\n",
    "    np.save(bottleneck_features_train_npy, bottleneck_features_train)\n",
    "\n",
    "    bottleneck_labels_train = to_categorical(generator.classes, num_classes=num_classes)\n",
    "    np.save(bottleneck_labels_train_npy, bottleneck_labels_train)\n",
    "    \n",
    "else:\n",
    "    bottleneck_features_train = np.load(bottleneck_features_train_npy)\n",
    "    bottleneck_labels_train = np.load(bottleneck_labels_train_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 341 images belonging to 21 classes.\n",
      "22/22 [==============================] - 5s 234ms/step\n"
     ]
    }
   ],
   "source": [
    "if not exists(bottleneck_features_valid_npy):\n",
    "    # Generate bottleneck features for validation data, and accompanying labels\n",
    "    generator = datagen.flow_from_directory(\n",
    "        valid_folder,\n",
    "        target_size=(INPUT_SIZE, INPUT_SIZE),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='sparse',\n",
    "        shuffle=False)\n",
    "\n",
    "    bottleneck_features_valid = base_model.predict_generator(generator, verbose = 1)\n",
    "    np.save(bottleneck_features_valid_npy, bottleneck_features_valid)\n",
    "\n",
    "    bottleneck_labels_valid = to_categorical(generator.classes, num_classes=num_classes)\n",
    "    np.save(bottleneck_labels_valid_npy, bottleneck_labels_valid)\n",
    "\n",
    "else:\n",
    "    bottleneck_features_valid = np.load(bottleneck_features_valid_npy)\n",
    "    bottleneck_labels_valid = np.load(bottleneck_labels_valid_npy)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create and train top-model with bottleneck features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model = Sequential()\n",
    "\n",
    "# Based on https://gist.github.com/fchollet/7eb39b44eb9e16e59632d25fb3119975\n",
    "# top_model.add(Flatten(input_shape=bottleneck_features_train.shape[1:]))\n",
    "# top_model.add(Dense(256, activation='relu'))\n",
    "# top_model.add(Dropout(0.5))\n",
    "\n",
    "# https://www.kaggle.com/abnera/transfer-learning-keras-xception-cnn\n",
    "# top_model.add(GlobalAveragePooling2D(input_shape=bottleneck_features_train.shape[1:]))\n",
    "\n",
    "# Based on https://www.depends-on-the-definition.com/transfer-learning-for-dog-breed-identification/\n",
    "top_model.add(BatchNormalization(input_shape=bottleneck_features_train.shape[1:]))\n",
    "top_model.add(GlobalAveragePooling2D())\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(1024, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    " \n",
    "top_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "top_model.compile(optimizer=SGD(nesterov=True),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/bckenstler/CLR for Cyclical Learning Rate\n",
    "from keras.callbacks import *\n",
    "\n",
    "class CyclicLR(Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or \n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "    \n",
    "    # Example\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    \n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```    \n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore \n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where \n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored \n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on \n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 947 samples, validate on 341 samples\n",
      "Epoch 1/50\n",
      "947/947 [==============================] - 1s 1ms/step - loss: 0.0484 - acc: 0.9937 - val_loss: 0.2235 - val_acc: 0.9238\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.92375, saving model to models/2018-05-10_top_model.h5\n",
      "Epoch 2/50\n",
      "947/947 [==============================] - 1s 1ms/step - loss: 0.0503 - acc: 0.9894 - val_loss: 0.2219 - val_acc: 0.9296\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.92375 to 0.92962, saving model to models/2018-05-10_top_model.h5\n",
      "Epoch 3/50\n",
      "947/947 [==============================] - 1s 1ms/step - loss: 0.0559 - acc: 0.9873 - val_loss: 0.2186 - val_acc: 0.9296\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.92962\n",
      "Epoch 4/50\n",
      "947/947 [==============================] - 1s 1ms/step - loss: 0.0478 - acc: 0.9905 - val_loss: 0.2193 - val_acc: 0.9296\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.92962\n",
      "Epoch 5/50\n",
      "947/947 [==============================] - 1s 1ms/step - loss: 0.0495 - acc: 0.9926 - val_loss: 0.2221 - val_acc: 0.9296\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.92962\n",
      "Epoch 6/50\n",
      "947/947 [==============================] - 1s 1ms/step - loss: 0.0454 - acc: 0.9937 - val_loss: 0.2244 - val_acc: 0.9296\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.92962\n",
      "Epoch 7/50\n",
      "947/947 [==============================] - 1s 1ms/step - loss: 0.0354 - acc: 0.9979 - val_loss: 0.2219 - val_acc: 0.9267\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.92962\n",
      "Epoch 8/50\n",
      "947/947 [==============================] - 1s 1ms/step - loss: 0.0383 - acc: 0.9926 - val_loss: 0.2207 - val_acc: 0.9238\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.92962\n",
      "Epoch 00008: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbf3cc16e80>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# save weights of best training epoch: monitor either val_loss or val_acc\n",
    "STAMP = \"{}_top_model\".format(datetime.datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "top_model_weights_path = \"models/{}.h5\".format(STAMP)\n",
    "\n",
    "# Authors suggest setting step_size = (2-8) x (training iterations in epoch)\n",
    "step_size = 2000\n",
    "clr = CyclicLR(base_lr=0.008,\n",
    "               max_lr=0.03,\n",
    "               step_size=step_size,\n",
    "               mode='exp_range',\n",
    "               gamma=0.99994)\n",
    "\n",
    "callbacks_list = [\n",
    "    ModelCheckpoint(top_model_weights_path, monitor='val_acc', verbose=1, save_best_only=True),\n",
    "    EarlyStopping(monitor='val_loss', patience=5, verbose=1),\n",
    "    clr\n",
    "]\n",
    "\n",
    "top_model.fit(bottleneck_features_train, bottleneck_labels_train,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=top_model_epochs,\n",
    "              callbacks=callbacks_list,\n",
    "              validation_data=(bottleneck_features_valid, bottleneck_labels_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup data to flow from directory with augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 947 images belonging to 21 classes.\n",
      "Found 341 images belonging to 21 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input_xception,\n",
    "                                   rotation_range=45,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.25,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "train_generator = train_datagen.flow_from_directory(directory=train_folder,\n",
    "                                                    target_size=(INPUT_SIZE,INPUT_SIZE),\n",
    "                                                    batch_size=BATCH_SIZE,\n",
    "                                                    class_mode='categorical')\n",
    "\n",
    "valid_datagen = ImageDataGenerator(preprocessing_function=preprocess_input_xception)\n",
    "valid_generator = valid_datagen.flow_from_directory(directory=valid_folder,\n",
    "                                                    target_size=(INPUT_SIZE,INPUT_SIZE),\n",
    "                                                    batch_size=BATCH_SIZE,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create and fine complete model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 299, 299, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 149, 149, 32) 864         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1_bn (BatchNormaliza (None, 149, 149, 32) 128         block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1_act (Activation)   (None, 149, 149, 32) 0           block1_conv1_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 147, 147, 64) 18432       block1_conv1_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2_bn (BatchNormaliza (None, 147, 147, 64) 256         block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2_act (Activation)   (None, 147, 147, 64) 0           block1_conv2_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv1 (SeparableConv2 (None, 147, 147, 128 8768        block1_conv2_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv1_bn (BatchNormal (None, 147, 147, 128 512         block2_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2_act (Activation (None, 147, 147, 128 0           block2_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2 (SeparableConv2 (None, 147, 147, 128 17536       block2_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2_bn (BatchNormal (None, 147, 147, 128 512         block2_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 74, 74, 128)  8192        block1_conv2_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 74, 74, 128)  0           block2_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 74, 74, 128)  512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 74, 74, 128)  0           block2_pool[0][0]                \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1_act (Activation (None, 74, 74, 128)  0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1 (SeparableConv2 (None, 74, 74, 256)  33920       block3_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1_bn (BatchNormal (None, 74, 74, 256)  1024        block3_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2_act (Activation (None, 74, 74, 256)  0           block3_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2 (SeparableConv2 (None, 74, 74, 256)  67840       block3_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2_bn (BatchNormal (None, 74, 74, 256)  1024        block3_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 37, 37, 256)  32768       add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 37, 37, 256)  0           block3_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 37, 37, 256)  1024        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 37, 37, 256)  0           block3_pool[0][0]                \n",
      "                                                                 batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1_act (Activation (None, 37, 37, 256)  0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1 (SeparableConv2 (None, 37, 37, 728)  188672      block4_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1_bn (BatchNormal (None, 37, 37, 728)  2912        block4_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2_act (Activation (None, 37, 37, 728)  0           block4_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2 (SeparableConv2 (None, 37, 37, 728)  536536      block4_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2_bn (BatchNormal (None, 37, 37, 728)  2912        block4_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 19, 19, 728)  186368      add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, 19, 19, 728)  0           block4_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 19, 19, 728)  2912        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 19, 19, 728)  0           block4_pool[0][0]                \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1_act (Activation (None, 19, 19, 728)  0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block5_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block5_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2_act (Activation (None, 19, 19, 728)  0           block5_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block5_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block5_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3_act (Activation (None, 19, 19, 728)  0           block5_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block5_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block5_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 19, 19, 728)  0           block5_sepconv3_bn[0][0]         \n",
      "                                                                 add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1_act (Activation (None, 19, 19, 728)  0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block6_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block6_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2_act (Activation (None, 19, 19, 728)  0           block6_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block6_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block6_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3_act (Activation (None, 19, 19, 728)  0           block6_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block6_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block6_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 19, 19, 728)  0           block6_sepconv3_bn[0][0]         \n",
      "                                                                 add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1_act (Activation (None, 19, 19, 728)  0           add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block7_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block7_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2_act (Activation (None, 19, 19, 728)  0           block7_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block7_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block7_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3_act (Activation (None, 19, 19, 728)  0           block7_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block7_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block7_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 19, 19, 728)  0           block7_sepconv3_bn[0][0]         \n",
      "                                                                 add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1_act (Activation (None, 19, 19, 728)  0           add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block8_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block8_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2_act (Activation (None, 19, 19, 728)  0           block8_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block8_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block8_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3_act (Activation (None, 19, 19, 728)  0           block8_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block8_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block8_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 19, 19, 728)  0           block8_sepconv3_bn[0][0]         \n",
      "                                                                 add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1_act (Activation (None, 19, 19, 728)  0           add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block9_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block9_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2_act (Activation (None, 19, 19, 728)  0           block9_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block9_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block9_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3_act (Activation (None, 19, 19, 728)  0           block9_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block9_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block9_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 19, 19, 728)  0           block9_sepconv3_bn[0][0]         \n",
      "                                                                 add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1_act (Activatio (None, 19, 19, 728)  0           add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1 (SeparableConv (None, 19, 19, 728)  536536      block10_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1_bn (BatchNorma (None, 19, 19, 728)  2912        block10_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2_act (Activatio (None, 19, 19, 728)  0           block10_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2 (SeparableConv (None, 19, 19, 728)  536536      block10_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2_bn (BatchNorma (None, 19, 19, 728)  2912        block10_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3_act (Activatio (None, 19, 19, 728)  0           block10_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3 (SeparableConv (None, 19, 19, 728)  536536      block10_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3_bn (BatchNorma (None, 19, 19, 728)  2912        block10_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 19, 19, 728)  0           block10_sepconv3_bn[0][0]        \n",
      "                                                                 add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1_act (Activatio (None, 19, 19, 728)  0           add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1 (SeparableConv (None, 19, 19, 728)  536536      block11_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1_bn (BatchNorma (None, 19, 19, 728)  2912        block11_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2_act (Activatio (None, 19, 19, 728)  0           block11_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2 (SeparableConv (None, 19, 19, 728)  536536      block11_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2_bn (BatchNorma (None, 19, 19, 728)  2912        block11_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3_act (Activatio (None, 19, 19, 728)  0           block11_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3 (SeparableConv (None, 19, 19, 728)  536536      block11_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3_bn (BatchNorma (None, 19, 19, 728)  2912        block11_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 19, 19, 728)  0           block11_sepconv3_bn[0][0]        \n",
      "                                                                 add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1_act (Activatio (None, 19, 19, 728)  0           add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1 (SeparableConv (None, 19, 19, 728)  536536      block12_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1_bn (BatchNorma (None, 19, 19, 728)  2912        block12_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2_act (Activatio (None, 19, 19, 728)  0           block12_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2 (SeparableConv (None, 19, 19, 728)  536536      block12_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2_bn (BatchNorma (None, 19, 19, 728)  2912        block12_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3_act (Activatio (None, 19, 19, 728)  0           block12_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3 (SeparableConv (None, 19, 19, 728)  536536      block12_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3_bn (BatchNorma (None, 19, 19, 728)  2912        block12_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 19, 19, 728)  0           block12_sepconv3_bn[0][0]        \n",
      "                                                                 add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1_act (Activatio (None, 19, 19, 728)  0           add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1 (SeparableConv (None, 19, 19, 728)  536536      block13_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1_bn (BatchNorma (None, 19, 19, 728)  2912        block13_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2_act (Activatio (None, 19, 19, 728)  0           block13_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2 (SeparableConv (None, 19, 19, 1024) 752024      block13_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2_bn (BatchNorma (None, 19, 19, 1024) 4096        block13_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 10, 10, 1024) 745472      add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_pool (MaxPooling2D)     (None, 10, 10, 1024) 0           block13_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 10, 10, 1024) 4096        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 10, 10, 1024) 0           block13_pool[0][0]               \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1 (SeparableConv (None, 10, 10, 1536) 1582080     add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1_bn (BatchNorma (None, 10, 10, 1536) 6144        block14_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1_act (Activatio (None, 10, 10, 1536) 0           block14_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2 (SeparableConv (None, 10, 10, 2048) 3159552     block14_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2_bn (BatchNorma (None, 10, 10, 2048) 8192        block14_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2_act (Activatio (None, 10, 10, 2048) 0           block14_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 21)           2127893     block14_sepconv2_act[0][0]       \n",
      "==================================================================================================\n",
      "Total params: 22,989,373\n",
      "Trainable params: 6,872,597\n",
      "Non-trainable params: 16,116,776\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# base_model = xception.Xception(weights='imagenet', include_top=False, input_tensor=Input(shape=(299,299,3)))\n",
    "\n",
    "# top_model = Sequential()\n",
    "# top_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
    "# top_model.add(Dense(256, activation='relu'))\n",
    "# top_model.add(Dropout(0.5))\n",
    "# top_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "top_model.load_weights(top_model_weights_path)\n",
    "\n",
    "# Stack top_model on top\n",
    "model = Model(inputs=base_model.input, outputs=top_model(base_model.output))\n",
    "\n",
    "#for i, layer in enumerate(model.layers):\n",
    "#    print ('Layer #: {}, Name: {}'.format(i, layer.name))\n",
    "\n",
    "for layer in model.layers[:126]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[126:]:\n",
    "    layer.trainable = True\n",
    "    \n",
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "model.compile(optimizer = SGD(lr=1e-4, momentum=0.9),\n",
    "              loss ='categorical_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# save weights of best training epoch: monitor either val_loss or val_acc\n",
    "STAMP = \"{}_complete_model\".format(datetime.datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "complete_model_weights_path = \"models/{}.h5\".format(STAMP)\n",
    "\n",
    "# Authors suggest setting step_size = (2-8) x (training iterations in epoch)\n",
    "step_size = 200\n",
    "clr = CyclicLR(base_lr=8*1e-5,\n",
    "               max_lr=4*1e-4,\n",
    "               step_size=step_size,\n",
    "               mode='exp_range',\n",
    "               gamma=0.99994)\n",
    "\n",
    "callbacks_list = [\n",
    "    ModelCheckpoint(complete_model_weights_path, monitor='val_acc', verbose=1, save_best_only=True),\n",
    "    EarlyStopping(monitor='val_loss', patience=5, verbose=1),\n",
    "    clr\n",
    "]\n",
    "\n",
    "hist = model.fit_generator(train_generator,\n",
    "                           steps_per_epoch=train_samples // BATCH_SIZE,\n",
    "                           epochs=complete_model_epochs,\n",
    "                           callbacks=callbacks_list,\n",
    "                           validation_data=valid_generator,\n",
    "                           validation_steps=valid_samples // BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results with original approach without random backgrounds - model is overfitting\n",
    "\n",
    "```\n",
    "Epoch 1/20\n",
    "13/13 [==============================] - 33s 3s/step - loss: 0.7600 - acc: 0.8881 - val_loss: 2.1108 - val_acc: 0.4146\n",
    "Epoch 2/20\n",
    "13/13 [==============================] - 32s 2s/step - loss: 0.3821 - acc: 0.9615 - val_loss: 2.0411 - val_acc: 0.4390\n",
    "Epoch 3/20\n",
    "13/13 [==============================] - 32s 2s/step - loss: 0.2718 - acc: 0.9712 - val_loss: 2.0189 - val_acc: 0.4497\n",
    "Epoch 4/20\n",
    "13/13 [==============================] - 31s 2s/step - loss: 0.1594 - acc: 0.9856 - val_loss: 2.0376 - val_acc: 0.4463\n",
    "Epoch 5/20\n",
    "13/13 [==============================] - 30s 2s/step - loss: 0.1112 - acc: 1.0000 - val_loss: 2.0218 - val_acc: 0.4497\n",
    "Epoch 6/20\n",
    "13/13 [==============================] - 30s 2s/step - loss: 0.0877 - acc: 1.0000 - val_loss: 2.0174 - val_acc: 0.4575\n",
    "Epoch 7/20\n",
    "13/13 [==============================] - 30s 2s/step - loss: 0.0691 - acc: 0.9904 - val_loss: 2.0520 - val_acc: 0.4507\n",
    "Epoch 8/20\n",
    "13/13 [==============================] - 30s 2s/step - loss: 0.0719 - acc: 0.9952 - val_loss: 2.0455 - val_acc: 0.4575\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results with random backgrounds - model is overfitting\n",
    "```\n",
    "Epoch 1/20\n",
    "70/70 [==============================] - 80s 1s/step - loss: 0.5809 - acc: 0.9080 - val_loss: 2.0572 - val_acc: 0.4507\n",
    "Epoch 2/20\n",
    "70/70 [==============================] - 79s 1s/step - loss: 0.0889 - acc: 0.9937 - val_loss: 2.0508 - val_acc: 0.4727\n",
    "Epoch 3/20\n",
    "70/70 [==============================] - 77s 1s/step - loss: 0.0460 - acc: 0.9964 - val_loss: 2.0762 - val_acc: 0.4883\n",
    "Epoch 4/20\n",
    "70/70 [==============================] - 80s 1s/step - loss: 0.0256 - acc: 0.9973 - val_loss: 2.1565 - val_acc: 0.4785\n",
    "Epoch 5/20\n",
    "70/70 [==============================] - 77s 1s/step - loss: 0.0152 - acc: 0.9982 - val_loss: 2.1780 - val_acc: 0.4795\n",
    "Epoch 6/20\n",
    "19/70 [=======>......................] - ETA: 34s - loss: 0.0086 - acc: 1.0000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input_xception)\n",
    "test_generator = test_datagen.flow_from_directory(directory='data/test',\n",
    "                                                  classes=classes,\n",
    "                                                  target_size=(INPUT_SIZE,INPUT_SIZE),\n",
    "                                                  batch_size=BATCH_SIZE,\n",
    "                                                  class_mode='sparse',\n",
    "                                                  shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 5s 242ms/step\n"
     ]
    }
   ],
   "source": [
    "generator = valid_generator\n",
    "ypred = model.predict_generator(generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = generator.filenames\n",
    "df = pd.DataFrame.from_dict(generator.class_indices, orient='index')\n",
    "class_list = df.sort_values(by=0).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of 341 images tested, top1 accuracy: 92.96% (24 wrong) and top3 accuracy: 98.83% (4 wrong)\n"
     ]
    }
   ],
   "source": [
    "true_classes = generator.classes\n",
    "pred1_classes = np.argsort(ypred, axis=1)[:,-1] # or np.argmax(ypred, axis=1)\n",
    "pred2_classes = np.argsort(ypred, axis=1)[:,-2]\n",
    "pred3_classes = np.argsort(ypred, axis=1)[:,-3]\n",
    "top1_error_idx = (true_classes != pred1_classes)\n",
    "top3_error_idx = (true_classes != pred1_classes) & (true_classes != pred2_classes) & (true_classes != pred3_classes)\n",
    "\n",
    "print (\"Of {} images tested, top1 accuracy: {:.2f}% ({} wrong) and top3 accuracy: {:.2f}% ({} wrong)\".format(\n",
    "        len(img_list),\n",
    "        (1-np.sum(top1_error_idx)/len(img_list))*100, np.sum(top1_error_idx),\n",
    "        (1-np.sum(top3_error_idx)/len(img_list))*100, np.sum(top3_error_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from itertools import compress\n",
    "\n",
    "error_idx = top3_error_idx\n",
    "\n",
    "for img_path, cat, pred1, pred2, pred3 in zip(list(compress(img_list, error_idx)),\n",
    "                                              [class_list[int(b)] for b in true_classes[error_idx]],\n",
    "                                              [class_list[int(b)] for b in pred1_classes[error_idx]],\n",
    "                                              [class_list[int(b)] for b in pred2_classes[error_idx]],\n",
    "                                              [class_list[int(b)] for b in pred3_classes[error_idx]]):\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    img = image.load_img(join(valid_folder, img_path), target_size=(INPUT_SIZE,INPUT_SIZE))\n",
    "    img = image.img_to_array(img)\n",
    "    ax.imshow(img / 255.)\n",
    "    ax.text(10, 250, 'P1: %s' % pred1, color='w', backgroundcolor='r', alpha=0.8)\n",
    "    ax.text(10, 270, 'P2: %s' % pred2, color='w', backgroundcolor='r', alpha=0.8)\n",
    "    ax.text(10, 290, 'P3: %s' % pred3, color='w', backgroundcolor='r', alpha=0.8)\n",
    "    ax.text(10, 310, 'LABEL: %s' % cat, color='k', backgroundcolor='g', alpha=0.8)\n",
    "    ax.axis('off')\n",
    "    plt.show()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_p36",
   "language": "python",
   "name": "tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
