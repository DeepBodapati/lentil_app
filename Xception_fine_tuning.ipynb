{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import listdir, makedirs, walk\n",
    "from os.path import isdir, isfile, join, exists, expanduser\n",
    "from tqdm import tqdm_notebook\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View underlying data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: 26, Training samples: 179, Validation samples: 51\n"
     ]
    }
   ],
   "source": [
    "train_folder      = 'data/train/'\n",
    "valid_folder      = 'data/validation/'\n",
    "weights_folder    = 'models/'\n",
    "\n",
    "bottleneck_features_train_npy = 'bottleneck_features_train.npy'\n",
    "bottleneck_features_valid_npy = 'bottleneck_features_valid.npy'\n",
    "bottleneck_labels_train_npy = 'bottleneck_labels_train.npy'\n",
    "bottleneck_labels_valid_npy = 'bottleneck_labels_valid.npy'\n",
    "\n",
    "top_model_epochs = 50\n",
    "complete_model_epochs = 20\n",
    "\n",
    "augmented_images_multiply_factor = 3\n",
    "\n",
    "# Create weights folder to save model weights\n",
    "if not exists(weights_folder):\n",
    "    makedirs(weights_folder)\n",
    "    \n",
    "ext = '.jpg'\n",
    "classes = [fldr for fldr in listdir(train_folder) if isdir(join(train_folder, fldr))]\n",
    "num_classes = len(classes)\n",
    "train_samples = sum([len(files) for r, d, files in walk(train_folder)])\n",
    "valid_samples = sum([len(files) for r, d, files in walk(valid_folder)])\n",
    "\n",
    "print (\"Classes: {}, Training samples: {}, Validation samples: {}\".format(num_classes, train_samples, valid_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "%matplotlib inline\n",
    "\n",
    "# options\n",
    "INPUT_SIZE = 299\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "fig = plt.figure(1, figsize=(16, 16))\n",
    "grid = ImageGrid(fig, 111, nrows_ncols=(3, 3), axes_pad=0.05)\n",
    "\n",
    "for i, cls in enumerate(np.random.choice(classes, 9)):\n",
    "    ax = grid[i]\n",
    "    img_file = np.random.choice(listdir(join(train_folder, cls)))\n",
    "    img = image.load_img(join(train_folder, cls, img_file), target_size=(INPUT_SIZE,INPUT_SIZE))\n",
    "    img = image.img_to_array(img)\n",
    "    ax.imshow(img / 255.)\n",
    "    ax.text(10, 200, 'LABEL: %s' % cls, color='k', backgroundcolor='w', alpha=0.8)\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapting [Keras example](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) for Xception model with multiclass classification\n",
    "\n",
    "1. Create base model and save bottleneck features\n",
    "2. Train top-model with bottleneck features\n",
    "3. Setup data to flow from directory with augmentation\n",
    "4. Fine tune base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.layers import Dropout, Flatten, Dense, Input, GlobalAveragePooling2D, BatchNormalization, deserialize\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications import xception\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create base model and save bottleneck features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating wrapper for input preprocessing from https://nbviewer.jupyter.org/gist/embanner/6149bba89c174af3bfd69537b72bca74 \n",
    "def preprocess_input_xception(x):\n",
    "    \"\"\"Wrapper around keras.applications.xception.preprocess_input()\n",
    "    to make it compatible for use with keras.preprocessing.image.ImageDataGenerator's\n",
    "    `preprocessing_function` argument.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : a numpy 3darray (a single image to be preprocessed)\n",
    "    \n",
    "    Note we cannot pass keras.applications.xception.preprocess_input()\n",
    "    directly to to keras.preprocessing.image.ImageDataGenerator's\n",
    "    `preprocessing_function` argument because the former expects a\n",
    "    4D tensor whereas the latter expects a 3D tensor. Hence the\n",
    "    existence of this wrapper.\n",
    "    \n",
    "    Returns a numpy 3darray (the preprocessed image).\n",
    "    \n",
    "    \"\"\"\n",
    "    from keras.applications.xception import preprocess_input\n",
    "    X = np.expand_dims(x, axis=0)\n",
    "    X = preprocess_input(X)\n",
    "    return X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = xception.Xception(weights='imagenet', include_top=False, input_tensor=Input(shape=(299,299,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not exists(bottleneck_features_train_npy):\n",
    "    # Set up data generator for bottleneck features - only need preprocessing (no augmentation)\n",
    "    datagen = ImageDataGenerator(preprocessing_function=preprocess_input_xception)\n",
    "\n",
    "    # Generate bottleneck features for training data, and accompanying labels\n",
    "    generator = datagen.flow_from_directory(\n",
    "        train_folder,\n",
    "        target_size=(INPUT_SIZE, INPUT_SIZE),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='sparse',\n",
    "        shuffle=False)\n",
    "\n",
    "    bottleneck_features_train = base_model.predict_generator(generator, verbose = 1)\n",
    "    np.save(bottleneck_features_train_npy, bottleneck_features_train)\n",
    "\n",
    "    bottleneck_labels_train = to_categorical(generator.classes, num_classes=num_classes)\n",
    "    np.save(bottleneck_labels_train_npy, bottleneck_labels_train)\n",
    "    \n",
    "else:\n",
    "    bottleneck_features_train = np.load(bottleneck_features_train_npy)\n",
    "    bottleneck_labels_train = np.load(bottleneck_labels_train_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not exists(bottleneck_features_valid_npy):\n",
    "    # Generate bottleneck features for validation data, and accompanying labels\n",
    "    generator = datagen.flow_from_directory(\n",
    "        valid_folder,\n",
    "        target_size=(INPUT_SIZE, INPUT_SIZE),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='sparse',\n",
    "        shuffle=False)\n",
    "\n",
    "    bottleneck_features_valid = base_model.predict_generator(generator, verbose = 1)\n",
    "    np.save(bottleneck_features_valid_npy, bottleneck_features_valid)\n",
    "\n",
    "    bottleneck_labels_valid = to_categorical(generator.classes, num_classes=num_classes)\n",
    "    np.save(bottleneck_labels_valid_npy, bottleneck_labels_valid)\n",
    "\n",
    "else:\n",
    "    bottleneck_features_valid = np.load(bottleneck_features_valid_npy)\n",
    "    bottleneck_labels_valid = np.load(bottleneck_labels_valid_npy)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create and train top-model with bottleneck features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model = Sequential()\n",
    "\n",
    "# Based on https://gist.github.com/fchollet/7eb39b44eb9e16e59632d25fb3119975\n",
    "# top_model.add(Flatten(input_shape=bottleneck_features_train.shape[1:]))\n",
    "# top_model.add(Dense(256, activation='relu'))\n",
    "# top_model.add(Dropout(0.5))\n",
    "\n",
    "# Based on https://www.kaggle.com/abnera/transfer-learning-keras-xception-cnn - the best performance\n",
    "# top_model.add(GlobalAveragePooling2D(input_shape=bottleneck_features_train.shape[1:]))\n",
    "\n",
    "# Based on https://www.depends-on-the-definition.com/transfer-learning-for-dog-breed-identification/\n",
    "top_model.add(BatchNormalization(input_shape=bottleneck_features_train.shape[1:]))\n",
    "top_model.add(GlobalAveragePooling2D())\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(1024, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    " \n",
    "top_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "top_model.compile(optimizer=SGD(nesterov=True),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/bckenstler/CLR for Cyclical Learning Rate\n",
    "from keras.callbacks import *\n",
    "\n",
    "class CyclicLR(Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or \n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "    \n",
    "    # Example\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    \n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```    \n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore \n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where \n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored \n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on \n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 179 samples, validate on 51 samples\n",
      "Epoch 1/50\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 3.6865 - acc: 0.0335 - val_loss: 3.0153 - val_acc: 0.2353\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.23529, saving model to models/2018-05-09_top_model.h5\n",
      "Epoch 2/50\n",
      "179/179 [==============================] - 0s 1ms/step - loss: 3.1204 - acc: 0.1397 - val_loss: 2.6635 - val_acc: 0.3725\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.23529 to 0.37255, saving model to models/2018-05-09_top_model.h5\n",
      "Epoch 3/50\n",
      "179/179 [==============================] - 0s 1ms/step - loss: 2.7096 - acc: 0.2011 - val_loss: 2.3215 - val_acc: 0.4510\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.37255 to 0.45098, saving model to models/2018-05-09_top_model.h5\n",
      "Epoch 4/50\n",
      "179/179 [==============================] - 0s 1ms/step - loss: 2.2517 - acc: 0.4190 - val_loss: 2.0115 - val_acc: 0.6275\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.45098 to 0.62745, saving model to models/2018-05-09_top_model.h5\n",
      "Epoch 5/50\n",
      "179/179 [==============================] - 0s 1ms/step - loss: 1.9339 - acc: 0.4693 - val_loss: 1.7426 - val_acc: 0.7255\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.62745 to 0.72549, saving model to models/2018-05-09_top_model.h5\n",
      "Epoch 6/50\n",
      "179/179 [==============================] - 0s 1ms/step - loss: 1.4364 - acc: 0.6648 - val_loss: 1.5168 - val_acc: 0.7647\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.72549 to 0.76471, saving model to models/2018-05-09_top_model.h5\n",
      "Epoch 7/50\n",
      "179/179 [==============================] - 0s 1ms/step - loss: 1.2487 - acc: 0.7374 - val_loss: 1.3161 - val_acc: 0.7451\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.76471\n",
      "Epoch 8/50\n",
      "179/179 [==============================] - 0s 1ms/step - loss: 1.0987 - acc: 0.7709 - val_loss: 1.1601 - val_acc: 0.7255\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.76471\n",
      "Epoch 9/50\n",
      "179/179 [==============================] - 0s 1ms/step - loss: 0.7708 - acc: 0.8827 - val_loss: 1.0615 - val_acc: 0.7647\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.76471\n",
      "Epoch 10/50\n",
      "179/179 [==============================] - 0s 1ms/step - loss: 0.5785 - acc: 0.9162 - val_loss: 0.9795 - val_acc: 0.7647\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.76471\n",
      "Epoch 11/50\n",
      "179/179 [==============================] - 0s 1ms/step - loss: 0.4794 - acc: 0.9441 - val_loss: 0.9285 - val_acc: 0.8039\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.76471 to 0.80392, saving model to models/2018-05-09_top_model.h5\n",
      "Epoch 12/50\n",
      "179/179 [==============================] - 0s 1ms/step - loss: 0.3993 - acc: 0.9665 - val_loss: 0.8692 - val_acc: 0.8039\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.80392\n",
      "Epoch 13/50\n",
      "179/179 [==============================] - 0s 1ms/step - loss: 0.3850 - acc: 0.9553 - val_loss: 0.8387 - val_acc: 0.7843\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.80392\n",
      "Epoch 14/50\n",
      "179/179 [==============================] - 0s 1ms/step - loss: 0.2597 - acc: 0.9832 - val_loss: 0.8234 - val_acc: 0.8039\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.80392 to 0.80392, saving model to models/2018-05-09_top_model.h5\n",
      "Epoch 15/50\n",
      "179/179 [==============================] - 0s 1ms/step - loss: 0.2492 - acc: 0.9832 - val_loss: 0.7855 - val_acc: 0.7843\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.80392\n",
      "Epoch 16/50\n",
      "179/179 [==============================] - 0s 1ms/step - loss: 0.1929 - acc: 0.9944 - val_loss: 0.7685 - val_acc: 0.8039\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.80392\n",
      "Epoch 17/50\n",
      "179/179 [==============================] - 0s 1ms/step - loss: 0.1963 - acc: 0.9944 - val_loss: 0.7349 - val_acc: 0.8235\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.80392 to 0.82353, saving model to models/2018-05-09_top_model.h5\n",
      "Epoch 18/50\n",
      "179/179 [==============================] - 0s 1ms/step - loss: 0.1675 - acc: 0.9832 - val_loss: 0.7107 - val_acc: 0.8431\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.82353 to 0.84314, saving model to models/2018-05-09_top_model.h5\n",
      "Epoch 19/50\n",
      "179/179 [==============================] - 0s 1ms/step - loss: 0.1392 - acc: 0.9944 - val_loss: 0.6991 - val_acc: 0.8431\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.84314\n",
      "Epoch 20/50\n",
      "179/179 [==============================] - 0s 1ms/step - loss: 0.0940 - acc: 1.0000 - val_loss: 0.7108 - val_acc: 0.8039\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.84314\n",
      "Epoch 21/50\n",
      "179/179 [==============================] - 0s 1ms/step - loss: 0.0955 - acc: 0.9944 - val_loss: 0.7177 - val_acc: 0.7843\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.84314\n",
      "Epoch 22/50\n",
      "179/179 [==============================] - 0s 1ms/step - loss: 0.0693 - acc: 0.9944 - val_loss: 0.6938 - val_acc: 0.8039\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.84314\n",
      "Epoch 23/50\n",
      "179/179 [==============================] - 0s 1ms/step - loss: 0.0920 - acc: 0.9944 - val_loss: 0.6682 - val_acc: 0.8431\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.84314\n",
      "Epoch 24/50\n",
      "179/179 [==============================] - 0s 1ms/step - loss: 0.0889 - acc: 0.9777 - val_loss: 0.6955 - val_acc: 0.8039\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.84314\n",
      "Epoch 25/50\n",
      "179/179 [==============================] - 0s 1ms/step - loss: 0.0908 - acc: 0.9944 - val_loss: 0.7619 - val_acc: 0.8039\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.84314\n",
      "Epoch 26/50\n",
      "179/179 [==============================] - 0s 1ms/step - loss: 0.0648 - acc: 1.0000 - val_loss: 0.6905 - val_acc: 0.8039\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.84314\n",
      "Epoch 27/50\n",
      "179/179 [==============================] - 0s 1ms/step - loss: 0.0631 - acc: 0.9944 - val_loss: 0.6664 - val_acc: 0.8039\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.84314\n",
      "Epoch 28/50\n",
      "179/179 [==============================] - 0s 1ms/step - loss: 0.0530 - acc: 1.0000 - val_loss: 0.6649 - val_acc: 0.8235\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.84314\n",
      "Epoch 29/50\n",
      "179/179 [==============================] - 0s 1ms/step - loss: 0.0683 - acc: 0.9888 - val_loss: 0.8031 - val_acc: 0.7647\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.84314\n",
      "Epoch 30/50\n",
      "179/179 [==============================] - 0s 1ms/step - loss: 0.0639 - acc: 0.9944 - val_loss: 0.7142 - val_acc: 0.8431\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.84314\n",
      "Epoch 31/50\n",
      "179/179 [==============================] - 0s 1ms/step - loss: 0.0544 - acc: 1.0000 - val_loss: 0.6808 - val_acc: 0.8039\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.84314\n",
      "Epoch 32/50\n",
      "179/179 [==============================] - 0s 1ms/step - loss: 0.0639 - acc: 0.9832 - val_loss: 0.7275 - val_acc: 0.8039\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.84314\n",
      "Epoch 33/50\n",
      "179/179 [==============================] - 0s 1ms/step - loss: 0.0604 - acc: 0.9944 - val_loss: 0.7693 - val_acc: 0.7647\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.84314\n",
      "Epoch 00033: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1e76678898>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# save weights of best training epoch: monitor either val_loss or val_acc\n",
    "STAMP = \"{}_top_model\".format(datetime.datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "top_model_weights_path = \"models/{}.h5\".format(STAMP)\n",
    "\n",
    "# Authors suggest setting step_size = (2-8) x (training iterations in epoch)\n",
    "step_size = 500\n",
    "clr = CyclicLR(base_lr=0.01,\n",
    "               max_lr=0.1,\n",
    "               step_size=step_size,\n",
    "               mode='exp_range',\n",
    "               gamma=0.99994)\n",
    "\n",
    "callbacks_list = [\n",
    "    ModelCheckpoint(top_model_weights_path, monitor='val_acc', verbose=1, save_best_only=True),\n",
    "    EarlyStopping(monitor='val_loss', patience=5, verbose=1),\n",
    "    clr\n",
    "]\n",
    "\n",
    "top_model.fit(bottleneck_features_train, bottleneck_labels_train,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=top_model_epochs,\n",
    "              callbacks=callbacks_list,\n",
    "              validation_data=(bottleneck_features_valid, bottleneck_labels_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup data to flow from directory with augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 179 images belonging to 26 classes.\n",
      "Found 51 images belonging to 26 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input_xception,\n",
    "                                   rotation_range=45,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.25,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "train_generator = train_datagen.flow_from_directory(directory=train_folder,\n",
    "                                                    target_size=(INPUT_SIZE,INPUT_SIZE),\n",
    "                                                    batch_size=BATCH_SIZE,\n",
    "                                                    class_mode='categorical')\n",
    "\n",
    "valid_datagen = ImageDataGenerator(preprocessing_function=preprocess_input_xception)\n",
    "valid_generator = valid_datagen.flow_from_directory(directory=valid_folder,\n",
    "                                                    target_size=(INPUT_SIZE,INPUT_SIZE),\n",
    "                                                    batch_size=BATCH_SIZE,\n",
    "                                                    class_mode='categorical',\n",
    "                                                   shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create and fine complete model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 299, 299, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 149, 149, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1_bn (BatchNormaliza (None, 149, 149, 32) 128         block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1_act (Activation)   (None, 149, 149, 32) 0           block1_conv1_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 147, 147, 64) 18432       block1_conv1_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2_bn (BatchNormaliza (None, 147, 147, 64) 256         block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2_act (Activation)   (None, 147, 147, 64) 0           block1_conv2_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv1 (SeparableConv2 (None, 147, 147, 128 8768        block1_conv2_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv1_bn (BatchNormal (None, 147, 147, 128 512         block2_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2_act (Activation (None, 147, 147, 128 0           block2_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2 (SeparableConv2 (None, 147, 147, 128 17536       block2_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2_bn (BatchNormal (None, 147, 147, 128 512         block2_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 74, 74, 128)  8192        block1_conv2_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 74, 74, 128)  0           block2_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 74, 74, 128)  512         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 74, 74, 128)  0           block2_pool[0][0]                \n",
      "                                                                 batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1_act (Activation (None, 74, 74, 128)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1 (SeparableConv2 (None, 74, 74, 256)  33920       block3_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1_bn (BatchNormal (None, 74, 74, 256)  1024        block3_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2_act (Activation (None, 74, 74, 256)  0           block3_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2 (SeparableConv2 (None, 74, 74, 256)  67840       block3_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2_bn (BatchNormal (None, 74, 74, 256)  1024        block3_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 37, 37, 256)  32768       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 37, 37, 256)  0           block3_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 37, 37, 256)  1024        conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 37, 37, 256)  0           block3_pool[0][0]                \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1_act (Activation (None, 37, 37, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1 (SeparableConv2 (None, 37, 37, 728)  188672      block4_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1_bn (BatchNormal (None, 37, 37, 728)  2912        block4_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2_act (Activation (None, 37, 37, 728)  0           block4_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2 (SeparableConv2 (None, 37, 37, 728)  536536      block4_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2_bn (BatchNormal (None, 37, 37, 728)  2912        block4_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 19, 19, 728)  186368      add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, 19, 19, 728)  0           block4_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 19, 19, 728)  2912        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 19, 19, 728)  0           block4_pool[0][0]                \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1_act (Activation (None, 19, 19, 728)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block5_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block5_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2_act (Activation (None, 19, 19, 728)  0           block5_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block5_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block5_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3_act (Activation (None, 19, 19, 728)  0           block5_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block5_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block5_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 19, 19, 728)  0           block5_sepconv3_bn[0][0]         \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1_act (Activation (None, 19, 19, 728)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block6_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block6_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2_act (Activation (None, 19, 19, 728)  0           block6_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block6_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block6_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3_act (Activation (None, 19, 19, 728)  0           block6_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block6_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block6_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 19, 19, 728)  0           block6_sepconv3_bn[0][0]         \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1_act (Activation (None, 19, 19, 728)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block7_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block7_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2_act (Activation (None, 19, 19, 728)  0           block7_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block7_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block7_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3_act (Activation (None, 19, 19, 728)  0           block7_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block7_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block7_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 19, 19, 728)  0           block7_sepconv3_bn[0][0]         \n",
      "                                                                 add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1_act (Activation (None, 19, 19, 728)  0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block8_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block8_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2_act (Activation (None, 19, 19, 728)  0           block8_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block8_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block8_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3_act (Activation (None, 19, 19, 728)  0           block8_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block8_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block8_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 19, 19, 728)  0           block8_sepconv3_bn[0][0]         \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1_act (Activation (None, 19, 19, 728)  0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block9_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block9_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2_act (Activation (None, 19, 19, 728)  0           block9_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block9_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block9_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3_act (Activation (None, 19, 19, 728)  0           block9_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block9_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block9_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 19, 19, 728)  0           block9_sepconv3_bn[0][0]         \n",
      "                                                                 add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1_act (Activatio (None, 19, 19, 728)  0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1 (SeparableConv (None, 19, 19, 728)  536536      block10_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1_bn (BatchNorma (None, 19, 19, 728)  2912        block10_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2_act (Activatio (None, 19, 19, 728)  0           block10_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2 (SeparableConv (None, 19, 19, 728)  536536      block10_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2_bn (BatchNorma (None, 19, 19, 728)  2912        block10_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3_act (Activatio (None, 19, 19, 728)  0           block10_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3 (SeparableConv (None, 19, 19, 728)  536536      block10_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3_bn (BatchNorma (None, 19, 19, 728)  2912        block10_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 19, 19, 728)  0           block10_sepconv3_bn[0][0]        \n",
      "                                                                 add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1_act (Activatio (None, 19, 19, 728)  0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1 (SeparableConv (None, 19, 19, 728)  536536      block11_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1_bn (BatchNorma (None, 19, 19, 728)  2912        block11_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2_act (Activatio (None, 19, 19, 728)  0           block11_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2 (SeparableConv (None, 19, 19, 728)  536536      block11_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2_bn (BatchNorma (None, 19, 19, 728)  2912        block11_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3_act (Activatio (None, 19, 19, 728)  0           block11_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3 (SeparableConv (None, 19, 19, 728)  536536      block11_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3_bn (BatchNorma (None, 19, 19, 728)  2912        block11_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 19, 19, 728)  0           block11_sepconv3_bn[0][0]        \n",
      "                                                                 add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1_act (Activatio (None, 19, 19, 728)  0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1 (SeparableConv (None, 19, 19, 728)  536536      block12_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1_bn (BatchNorma (None, 19, 19, 728)  2912        block12_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2_act (Activatio (None, 19, 19, 728)  0           block12_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2 (SeparableConv (None, 19, 19, 728)  536536      block12_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2_bn (BatchNorma (None, 19, 19, 728)  2912        block12_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3_act (Activatio (None, 19, 19, 728)  0           block12_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3 (SeparableConv (None, 19, 19, 728)  536536      block12_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3_bn (BatchNorma (None, 19, 19, 728)  2912        block12_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 19, 19, 728)  0           block12_sepconv3_bn[0][0]        \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1_act (Activatio (None, 19, 19, 728)  0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1 (SeparableConv (None, 19, 19, 728)  536536      block13_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1_bn (BatchNorma (None, 19, 19, 728)  2912        block13_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2_act (Activatio (None, 19, 19, 728)  0           block13_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2 (SeparableConv (None, 19, 19, 1024) 752024      block13_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2_bn (BatchNorma (None, 19, 19, 1024) 4096        block13_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 10, 10, 1024) 745472      add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_pool (MaxPooling2D)     (None, 10, 10, 1024) 0           block13_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 10, 10, 1024) 4096        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 10, 10, 1024) 0           block13_pool[0][0]               \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1 (SeparableConv (None, 10, 10, 1536) 1582080     add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1_bn (BatchNorma (None, 10, 10, 1536) 6144        block14_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1_act (Activatio (None, 10, 10, 1536) 0           block14_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2 (SeparableConv (None, 10, 10, 2048) 3159552     block14_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2_bn (BatchNorma (None, 10, 10, 2048) 8192        block14_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2_act (Activatio (None, 10, 10, 2048) 0           block14_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 26)           2133018     block14_sepconv2_act[0][0]       \n",
      "==================================================================================================\n",
      "Total params: 22,994,498\n",
      "Trainable params: 6,877,722\n",
      "Non-trainable params: 16,116,776\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# base_model = xception.Xception(weights='imagenet', include_top=False, input_tensor=Input(shape=(299,299,3)))\n",
    "\n",
    "# top_model = Sequential()\n",
    "# top_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
    "# top_model.add(Dense(256, activation='relu'))\n",
    "# top_model.add(Dropout(0.5))\n",
    "# top_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "top_model.load_weights(top_model_weights_path)\n",
    "\n",
    "# Stack top_model on top\n",
    "model = Model(inputs=base_model.input, outputs=top_model(base_model.output))\n",
    "\n",
    "#for i, layer in enumerate(model.layers):\n",
    "#    print ('Layer #: {}, Name: {}'.format(i, layer.name))\n",
    "\n",
    "for layer in model.layers[:126]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[126:]:\n",
    "    layer.trainable = True\n",
    "    \n",
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "11/11 [==============================] - 5s 431ms/step - loss: 1.1033 - acc: 0.7884 - val_loss: 0.7829 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.79167, saving model to models/2018-05-09_complete_model.h5\n",
      "Epoch 2/20\n",
      "11/11 [==============================] - 4s 359ms/step - loss: 0.9348 - acc: 0.7830 - val_loss: 0.7662 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.79167\n",
      "Epoch 3/20\n",
      "11/11 [==============================] - 4s 349ms/step - loss: 0.6514 - acc: 0.8745 - val_loss: 0.7393 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.79167\n",
      "Epoch 4/20\n",
      "11/11 [==============================] - 4s 332ms/step - loss: 0.5007 - acc: 0.8970 - val_loss: 0.7379 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.79167\n",
      "Epoch 5/20\n",
      "11/11 [==============================] - 4s 337ms/step - loss: 0.3964 - acc: 0.9148 - val_loss: 0.7011 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.79167\n",
      "Epoch 6/20\n",
      "11/11 [==============================] - 3s 282ms/step - loss: 0.4274 - acc: 0.9140 - val_loss: 0.7102 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.79167\n",
      "Epoch 7/20\n",
      "11/11 [==============================] - 4s 337ms/step - loss: 0.2303 - acc: 0.9489 - val_loss: 0.7550 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.79167\n",
      "Epoch 8/20\n",
      "11/11 [==============================] - 3s 317ms/step - loss: 0.2339 - acc: 0.9374 - val_loss: 0.7719 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.79167\n",
      "Epoch 9/20\n",
      "11/11 [==============================] - 3s 282ms/step - loss: 0.2369 - acc: 0.9712 - val_loss: 0.7114 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.79167\n",
      "Epoch 10/20\n",
      "11/11 [==============================] - 3s 305ms/step - loss: 0.2708 - acc: 0.9260 - val_loss: 0.6736 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.79167\n",
      "Epoch 11/20\n",
      "11/11 [==============================] - 4s 340ms/step - loss: 0.1470 - acc: 0.9602 - val_loss: 0.6131 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.79167\n",
      "Epoch 12/20\n",
      "11/11 [==============================] - 3s 309ms/step - loss: 0.1368 - acc: 0.9603 - val_loss: 0.5660 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.79167 to 0.81250, saving model to models/2018-05-09_complete_model.h5\n",
      "Epoch 13/20\n",
      "11/11 [==============================] - 3s 296ms/step - loss: 0.1076 - acc: 0.9886 - val_loss: 0.5548 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.81250\n",
      "Epoch 14/20\n",
      "11/11 [==============================] - 4s 356ms/step - loss: 0.1608 - acc: 0.9320 - val_loss: 0.4966 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.81250\n",
      "Epoch 15/20\n",
      "11/11 [==============================] - 4s 365ms/step - loss: 0.1066 - acc: 0.9716 - val_loss: 0.5647 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.81250\n",
      "Epoch 16/20\n",
      "11/11 [==============================] - 4s 319ms/step - loss: 0.1338 - acc: 0.9659 - val_loss: 0.5660 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.81250\n",
      "Epoch 17/20\n",
      "11/11 [==============================] - 4s 340ms/step - loss: 0.0716 - acc: 0.9830 - val_loss: 0.4520 - val_acc: 0.8542\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.81250 to 0.85417, saving model to models/2018-05-09_complete_model.h5\n",
      "Epoch 18/20\n",
      "11/11 [==============================] - 3s 288ms/step - loss: 0.0829 - acc: 0.9717 - val_loss: 0.3752 - val_acc: 0.8958\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.85417 to 0.89583, saving model to models/2018-05-09_complete_model.h5\n",
      "Epoch 19/20\n",
      "11/11 [==============================] - 4s 319ms/step - loss: 0.0691 - acc: 0.9830 - val_loss: 0.4521 - val_acc: 0.8542\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.89583\n",
      "Epoch 20/20\n",
      "11/11 [==============================] - 3s 312ms/step - loss: 0.0807 - acc: 0.9771 - val_loss: 0.5862 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.89583\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# save weights of best training epoch: monitor either val_loss or val_acc\n",
    "STAMP = \"{}_complete_model\".format(datetime.datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "complete_model_weights_path = \"models/{}.h5\".format(STAMP)\n",
    "\n",
    "# Authors suggest setting step_size = (2-8) x (training iterations in epoch)\n",
    "step_size = 1200\n",
    "clr = CyclicLR(base_lr=0.0002,\n",
    "               max_lr=0.002,\n",
    "               step_size=step_size,\n",
    "               mode='exp_range',\n",
    "               gamma=0.99994)\n",
    "\n",
    "callbacks_list = [\n",
    "    ModelCheckpoint(complete_model_weights_path, monitor='val_acc', verbose=1, save_best_only=True),\n",
    "    EarlyStopping(monitor='val_loss', patience=5, verbose=1),\n",
    "    clr\n",
    "]\n",
    "\n",
    "hist = model.fit_generator(train_generator,\n",
    "                           steps_per_epoch=train_samples // BATCH_SIZE,\n",
    "                           epochs=complete_model_epochs,\n",
    "                           callbacks=callbacks_list,\n",
    "                           validation_data=valid_generator,\n",
    "                           validation_steps=valid_samples // BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results with original approach without random backgrounds - model is overfitting\n",
    "\n",
    "```\n",
    "Epoch 1/20\n",
    "13/13 [==============================] - 33s 3s/step - loss: 0.7600 - acc: 0.8881 - val_loss: 2.1108 - val_acc: 0.4146\n",
    "\n",
    "Epoch 00001: val_acc improved from -inf to 0.41455, saving model to models/2018-05-08_complete_model.h5\n",
    "\n",
    "Epoch 2/20\n",
    "13/13 [==============================] - 32s 2s/step - loss: 0.3821 - acc: 0.9615 - val_loss: 2.0411 - val_acc: 0.4390\n",
    "\n",
    "Epoch 00002: val_acc improved from 0.41455 to 0.43896, saving model to models/2018-05-08_complete_model.h5\n",
    "\n",
    "Epoch 3/20\n",
    "13/13 [==============================] - 32s 2s/step - loss: 0.2718 - acc: 0.9712 - val_loss: 2.0189 - val_acc: 0.4497\n",
    "\n",
    "Epoch 00003: val_acc improved from 0.43896 to 0.44971, saving model to models/2018-05-08_complete_model.h5\n",
    "\n",
    "Epoch 4/20\n",
    "13/13 [==============================] - 31s 2s/step - loss: 0.1594 - acc: 0.9856 - val_loss: 2.0376 - val_acc: 0.4463\n",
    "\n",
    "Epoch 00004: val_acc did not improve from 0.44971\n",
    "\n",
    "Epoch 5/20\n",
    "13/13 [==============================] - 30s 2s/step - loss: 0.1112 - acc: 1.0000 - val_loss: 2.0218 - val_acc: 0.4497\n",
    "\n",
    "Epoch 00005: val_acc did not improve from 0.44971\n",
    "\n",
    "Epoch 6/20\n",
    "13/13 [==============================] - 30s 2s/step - loss: 0.0877 - acc: 1.0000 - val_loss: 2.0174 - val_acc: 0.4575\n",
    "\n",
    "Epoch 00006: val_acc improved from 0.44971 to 0.45752, saving model to models/2018-05-08_complete_model.h5\n",
    "\n",
    "Epoch 7/20\n",
    "13/13 [==============================] - 30s 2s/step - loss: 0.0691 - acc: 0.9904 - val_loss: 2.0520 - val_acc: 0.4507\n",
    "\n",
    "Epoch 00007: val_acc did not improve from 0.45752\n",
    "\n",
    "Epoch 8/20\n",
    "13/13 [==============================] - 30s 2s/step - loss: 0.0719 - acc: 0.9952 - val_loss: 2.0455 - val_acc: 0.4575\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results with random backgrounds - model is overfitting\n",
    "```\n",
    "Epoch 1/20\n",
    "70/70 [==============================] - 80s 1s/step - loss: 0.5809 - acc: 0.9080 - val_loss: 2.0572 - val_acc: 0.4507\n",
    "\n",
    "Epoch 00001: val_acc improved from -inf to 0.45068, saving model to models/2018-05-09_complete_model.h5\n",
    "Epoch 2/20\n",
    "70/70 [==============================] - 79s 1s/step - loss: 0.0889 - acc: 0.9937 - val_loss: 2.0508 - val_acc: 0.4727\n",
    "\n",
    "Epoch 00002: val_acc improved from 0.45068 to 0.47266, saving model to models/2018-05-09_complete_model.h5\n",
    "Epoch 3/20\n",
    "70/70 [==============================] - 77s 1s/step - loss: 0.0460 - acc: 0.9964 - val_loss: 2.0762 - val_acc: 0.4883\n",
    "\n",
    "Epoch 00003: val_acc improved from 0.47266 to 0.48828, saving model to models/2018-05-09_complete_model.h5\n",
    "Epoch 4/20\n",
    "70/70 [==============================] - 80s 1s/step - loss: 0.0256 - acc: 0.9973 - val_loss: 2.1565 - val_acc: 0.4785\n",
    "\n",
    "Epoch 00004: val_acc did not improve from 0.48828\n",
    "Epoch 5/20\n",
    "70/70 [==============================] - 77s 1s/step - loss: 0.0152 - acc: 0.9982 - val_loss: 2.1780 - val_acc: 0.4795\n",
    "\n",
    "Epoch 00005: val_acc did not improve from 0.48828\n",
    "Epoch 6/20\n",
    "19/70 [=======>......................] - ETA: 34s - loss: 0.0086 - acc: 1.0000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1848 images belonging to 26 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input_xception)\n",
    "test_generator = test_datagen.flow_from_directory(directory='imgs/ebay/',\n",
    "                                                  classes=classes,\n",
    "                                                  target_size=(INPUT_SIZE,INPUT_SIZE),\n",
    "                                                  batch_size=BATCH_SIZE,\n",
    "                                                  class_mode='sparse',\n",
    "                                                  shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 21s 185ms/step\n"
     ]
    }
   ],
   "source": [
    "generator = test_generator\n",
    "ypred = model.predict_generator(generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17, 17, 17, ...,  5,  5,  5])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(ypred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = generator.filenames\n",
    "df = pd.DataFrame.from_dict(generator.class_indices, orient='index')\n",
    "class_list = df.sort_values(by=0).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_classes = generator.classes\n",
    "pred_classes = np.argmax(ypred, axis=1)\n",
    "error_idx = (true_classes != pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0, ..., 25, 25, 25], dtype=int32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17, 17, 17, ...,  5,  5,  5])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from itertools import compress\n",
    "\n",
    "img_list = generator.filenames\n",
    "df = pd.DataFrame.from_dict(generator.class_indices, orient='index')\n",
    "class_list = df.sort_values(by=0).index.tolist()\n",
    "\n",
    "true_classes = generator.classes\n",
    "pred_classes = np.argmax(ypred, axis=1)\n",
    "error_idx = (true_classes != pred_classes)\n",
    "\n",
    "for img_path, cat, pred in zip(list(compress(img_list, error_idx)),\n",
    "                               [class_list[int(b)] for b in true_classes[error_idx]],\n",
    "                               [class_list[int(b)] for b in pred_classes[error_idx]]):\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    img = image.load_img(join('imgs/ebay', img_path), target_size=(INPUT_SIZE,INPUT_SIZE))\n",
    "    img = image.img_to_array(img)\n",
    "    ax.imshow(img / 255.)\n",
    "    ax.text(10, 250, 'Prediction: %s' % pred, color='w', backgroundcolor='r', alpha=0.8)\n",
    "    ax.text(10, 270, 'LABEL: %s' % cat, color='k', backgroundcolor='g', alpha=0.8)\n",
    "    ax.axis('off')\n",
    "    plt.show()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.051406926406926456"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-np.sum(error_idx)/len(img_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_p36",
   "language": "python",
   "name": "tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
