{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning and fine-tuning with Keras (InceptionV3 model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is loosely based on [Keras example](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html). This is a fine-grain, multi-class classifier problem, with few-shot learning (we have less than 10 training samples for a lot of our classes). \n",
    "\n",
    "To achieve this, we'll leverage the pre-built InceptionV3 model for the earlier layers, and stack a smaller model on top of this.  \n",
    "\n",
    "Here are the key steps:\n",
    "1. First, we create the InceptionV3 model (we'll call this the 'base model') with pre-built weights; the top layer is omitted \n",
    "2. Next, we generate the bottlenck features and accompanying training / validation labels (this will be useful when training the model in steps 5 and y)\n",
    "3. Next, we'll stack a much smaller model on top of the 'base' model (the smaller model will be referred to as 'top model')\n",
    "4. To establish a baseline, we'll test this untrained model (since it has random weights, this will have very poor performance)\n",
    "5. Now, we will train the 'top model' alone with relatively large learning rate using the bottleneck features from step 2 (the best performing weights are automatically saved) \n",
    "6. We will retest the entire model with the trained 'top model' weights\n",
    "7. Now, we will set up training data to flow from directory with real-time augmentation \n",
    "8. With this augmented training data, we will fine-tune the top layers of the 'base model' and all the layers in the 'top model' (the best performing weights are automatically saved) \n",
    "9. Finally, we'll retest the entire model with the fine-tuned weights\n",
    "\n",
    "First, take care of imports and initial setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import listdir, makedirs, walk\n",
    "from os.path import isdir, isfile, join, exists, expanduser\n",
    "from tqdm import tqdm_notebook\n",
    "import datetime\n",
    "from itertools import compress\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.layers import Dropout, Flatten, Dense, Input, GlobalAveragePooling2D, BatchNormalization, deserialize\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications import inception_v3\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.callbacks import *\n",
    "\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: 33, Training samples: 1725, Validation samples: 418, Test samples: 32\n"
     ]
    }
   ],
   "source": [
    "# inception_v3 needs images of size 299 x 299 \n",
    "INPUT_SIZE = 299\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_folder      = 'data/train/'\n",
    "valid_folder      = 'data/validation/'\n",
    "test_folder       = 'imgs/test'\n",
    "weights_folder    = 'models/'\n",
    "\n",
    "bottleneck_features_train_npy = 'bottleneck_features_train.npy'\n",
    "bottleneck_features_valid_npy = 'bottleneck_features_valid.npy'\n",
    "bottleneck_labels_train_npy = 'bottleneck_labels_train.npy'\n",
    "bottleneck_labels_valid_npy = 'bottleneck_labels_valid.npy'\n",
    "\n",
    "top_model_epochs = 50\n",
    "complete_model_epochs = 50\n",
    "\n",
    "# Number of augmented images to generate for each real image\n",
    "augmented_images_multiply_factor = 3\n",
    "\n",
    "# Create weights folder to save model weights\n",
    "if not exists(weights_folder):\n",
    "    makedirs(weights_folder)\n",
    "    \n",
    "ext = '.jpg'\n",
    "classes = [fldr for fldr in listdir(train_folder) if isdir(join(train_folder, fldr))]\n",
    "num_classes = len(classes)\n",
    "train_samples = sum([len(files) for r, d, files in walk(train_folder)])\n",
    "valid_samples = sum([len(files) for r, d, files in walk(valid_folder)])\n",
    "test_samples = sum([len(files) for r, d, files in walk(test_folder)])\n",
    "\n",
    "print (\"Classes: {}, Training samples: {}, Validation samples: {}, Test samples: {}\".format(num_classes, train_samples, valid_samples, test_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick view of the underlying data (images chosen at random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(16, 16))\n",
    "grid = ImageGrid(fig, 111, nrows_ncols=(3, 3), axes_pad=0.05)\n",
    "\n",
    "for i, cls in enumerate(np.random.choice(classes, 9)):\n",
    "    ax = grid[i]\n",
    "    img_file = np.random.choice(listdir(join(train_folder, cls)))\n",
    "    img = image.load_img(join(train_folder, cls, img_file), target_size=(INPUT_SIZE,INPUT_SIZE))\n",
    "    img = image.img_to_array(img)\n",
    "    ax.imshow(img / 255.)\n",
    "    ax.text(10, 200, 'LABEL: %s' % cls, color='k', backgroundcolor='w', alpha=0.8)\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create 'base model' without top layer and Imagenet weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = inception_v3.InceptionV3(weights='imagenet', include_top=False, input_tensor=Input(shape=(INPUT_SIZE,INPUT_SIZE,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate bottleneck features & labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1725 images belonging to 33 classes.\n",
      "108/108 [==============================] - 15s 135ms/step\n"
     ]
    }
   ],
   "source": [
    "if not exists(bottleneck_features_train_npy):\n",
    "    # Set up data generator for bottleneck features - only need preprocessing (no augmentation)\n",
    "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    # Generate bottleneck features for training data, and accompanying labels\n",
    "    generator = datagen.flow_from_directory(\n",
    "        train_folder,\n",
    "        target_size=(INPUT_SIZE, INPUT_SIZE),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='sparse',\n",
    "        shuffle=False)\n",
    "\n",
    "    bottleneck_features_train = base_model.predict_generator(generator, verbose = 1)\n",
    "    np.save(bottleneck_features_train_npy, bottleneck_features_train)\n",
    "\n",
    "    bottleneck_labels_train = to_categorical(generator.classes, num_classes=num_classes)\n",
    "    np.save(bottleneck_labels_train_npy, bottleneck_labels_train)\n",
    "    \n",
    "else:\n",
    "    bottleneck_features_train = np.load(bottleneck_features_train_npy)\n",
    "    bottleneck_labels_train = np.load(bottleneck_labels_train_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 418 images belonging to 33 classes.\n",
      "27/27 [==============================] - 3s 127ms/step\n"
     ]
    }
   ],
   "source": [
    "if not exists(bottleneck_features_valid_npy):\n",
    "    # Generate bottleneck features for validation data, and accompanying labels\n",
    "    generator = datagen.flow_from_directory(\n",
    "        valid_folder,\n",
    "        target_size=(INPUT_SIZE, INPUT_SIZE),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='sparse',\n",
    "        shuffle=False)\n",
    "\n",
    "    bottleneck_features_valid = base_model.predict_generator(generator, verbose = 1)\n",
    "    np.save(bottleneck_features_valid_npy, bottleneck_features_valid)\n",
    "\n",
    "    bottleneck_labels_valid = to_categorical(generator.classes, num_classes=num_classes)\n",
    "    np.save(bottleneck_labels_valid_npy, bottleneck_labels_valid)\n",
    "\n",
    "else:\n",
    "    bottleneck_features_valid = np.load(bottleneck_features_valid_npy)\n",
    "    bottleneck_labels_valid = np.load(bottleneck_labels_valid_npy)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create smaller 'top' model and stack on top of 'base' model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on https://www.depends-on-the-definition.com/transfer-learning-for-dog-breed-identification/\n",
    "\n",
    "top_model = Sequential()\n",
    "top_model.add(BatchNormalization(input_shape=bottleneck_features_train.shape[1:]))\n",
    "top_model.add(GlobalAveragePooling2D())\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(1024, activation='relu', kernel_initializer='random_uniform',\n",
    "                bias_initializer='zeros'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(num_classes, kernel_initializer='random_uniform',\n",
    "                bias_initializer='zeros', activation='softmax'))\n",
    "\n",
    "top_model.compile(optimizer=SGD(nesterov=True),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 299, 299, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 149, 149, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 149, 149, 32) 96          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 149, 149, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 147, 147, 32) 9216        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 147, 147, 32) 96          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 147, 147, 32) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 147, 147, 64) 18432       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 147, 147, 64) 192         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 147, 147, 64) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 73, 73, 64)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 73, 73, 80)   5120        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 73, 73, 80)   240         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 73, 73, 80)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 71, 71, 192)  138240      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 71, 71, 192)  576         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 71, 71, 192)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 35, 35, 192)  0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 35, 35, 64)   192         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 35, 35, 64)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 35, 35, 48)   9216        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 35, 35, 96)   55296       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 35, 35, 48)   144         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 35, 35, 96)   288         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 35, 35, 48)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 35, 35, 96)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 35, 35, 192)  0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 35, 35, 64)   76800       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 35, 35, 96)   82944       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 35, 35, 32)   6144        average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 35, 35, 64)   192         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 35, 35, 64)   192         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 35, 35, 96)   288         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 35, 35, 32)   96          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 35, 35, 64)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 35, 35, 64)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 35, 35, 96)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 35, 35, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 35, 35, 256)  0           activation_6[0][0]               \n",
      "                                                                 activation_8[0][0]               \n",
      "                                                                 activation_11[0][0]              \n",
      "                                                                 activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 35, 35, 64)   192         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 35, 35, 64)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 35, 35, 48)   12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 35, 35, 96)   55296       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 35, 35, 48)   144         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 35, 35, 96)   288         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 35, 35, 48)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 35, 35, 96)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 35, 35, 256)  0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 35, 35, 64)   76800       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 35, 35, 96)   82944       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 35, 35, 64)   16384       average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 35, 35, 64)   192         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 35, 35, 64)   192         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 35, 35, 96)   288         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 35, 35, 64)   192         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 35, 35, 64)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 35, 35, 64)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 35, 35, 96)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 35, 35, 64)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 35, 35, 288)  0           activation_13[0][0]              \n",
      "                                                                 activation_15[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "                                                                 activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 35, 35, 64)   192         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 35, 35, 64)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 35, 35, 48)   13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 35, 35, 96)   55296       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 35, 35, 48)   144         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 35, 35, 96)   288         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 35, 35, 48)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 35, 35, 96)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 35, 35, 288)  0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 35, 35, 64)   76800       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 35, 35, 96)   82944       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 35, 35, 64)   18432       average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 35, 35, 64)   192         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 35, 35, 64)   192         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 35, 35, 96)   288         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 35, 35, 64)   192         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 35, 35, 64)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 35, 35, 64)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 35, 35, 96)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 35, 35, 64)   0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 35, 35, 288)  0           activation_20[0][0]              \n",
      "                                                                 activation_22[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "                                                                 activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 35, 35, 64)   18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 35, 35, 64)   192         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 35, 35, 64)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 35, 35, 96)   55296       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 35, 35, 96)   288         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 35, 35, 96)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 17, 17, 384)  995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 17, 17, 96)   82944       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 17, 17, 384)  1152        conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 17, 17, 96)   288         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 17, 17, 384)  0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 17, 17, 96)   0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 17, 17, 288)  0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 17, 17, 768)  0           activation_27[0][0]              \n",
      "                                                                 activation_30[0][0]              \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 17, 17, 128)  384         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 17, 17, 128)  0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 17, 17, 128)  114688      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 17, 17, 128)  384         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 17, 17, 128)  0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 17, 17, 128)  114688      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 17, 17, 128)  384         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 17, 17, 128)  384         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 17, 17, 128)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 17, 17, 128)  0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 17, 17, 128)  114688      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 17, 17, 128)  114688      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 17, 17, 128)  384         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 17, 17, 128)  384         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 17, 17, 128)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 17, 17, 128)  0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 17, 17, 768)  0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 17, 17, 192)  147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 17, 17, 192)  172032      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 17, 17, 192)  172032      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 17, 17, 192)  576         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 17, 17, 192)  576         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 17, 17, 192)  576         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 17, 17, 192)  576         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 17, 17, 192)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 17, 17, 192)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 17, 17, 192)  0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 17, 17, 192)  0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 17, 17, 768)  0           activation_31[0][0]              \n",
      "                                                                 activation_34[0][0]              \n",
      "                                                                 activation_39[0][0]              \n",
      "                                                                 activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 17, 17, 160)  480         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 17, 17, 160)  0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 17, 17, 160)  179200      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 17, 17, 160)  480         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 17, 17, 160)  0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 17, 17, 160)  179200      activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 17, 17, 160)  480         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 17, 17, 160)  480         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 17, 17, 160)  0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 17, 17, 160)  0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 17, 17, 160)  179200      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 17, 17, 160)  179200      activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 17, 17, 160)  480         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 17, 17, 160)  480         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 17, 17, 160)  0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 17, 17, 160)  0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 17, 17, 768)  0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 17, 17, 192)  147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 17, 17, 192)  215040      activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 17, 17, 192)  215040      activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 17, 17, 192)  576         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 17, 17, 192)  576         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 17, 17, 192)  576         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 17, 17, 192)  576         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 17, 17, 192)  0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 17, 17, 192)  0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 17, 17, 192)  0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 17, 17, 192)  0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 17, 17, 768)  0           activation_41[0][0]              \n",
      "                                                                 activation_44[0][0]              \n",
      "                                                                 activation_49[0][0]              \n",
      "                                                                 activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 17, 17, 160)  480         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 17, 17, 160)  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 17, 17, 160)  179200      activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 17, 17, 160)  480         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 17, 17, 160)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 17, 17, 160)  179200      activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 17, 17, 160)  480         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 17, 17, 160)  480         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 17, 17, 160)  0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 17, 17, 160)  0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 17, 17, 160)  179200      activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 17, 17, 160)  179200      activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 17, 17, 160)  480         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 17, 17, 160)  480         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 17, 17, 160)  0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 17, 17, 160)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePoo (None, 17, 17, 768)  0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 17, 17, 192)  147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 17, 17, 192)  215040      activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 17, 17, 192)  215040      activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 17, 17, 192)  576         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 17, 17, 192)  576         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 17, 17, 192)  576         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 17, 17, 192)  576         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 17, 17, 192)  0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 17, 17, 192)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 17, 17, 192)  0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 17, 17, 192)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 17, 17, 768)  0           activation_51[0][0]              \n",
      "                                                                 activation_54[0][0]              \n",
      "                                                                 activation_59[0][0]              \n",
      "                                                                 activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 17, 17, 192)  576         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 17, 17, 192)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 17, 17, 192)  258048      activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 17, 17, 192)  576         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 17, 17, 192)  0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 17, 17, 192)  258048      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 17, 17, 192)  576         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 17, 17, 192)  576         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 17, 17, 192)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 17, 17, 192)  0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 17, 17, 192)  258048      activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 17, 17, 192)  258048      activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 17, 17, 192)  576         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 17, 17, 192)  576         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 17, 17, 192)  0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 17, 17, 192)  0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, 17, 17, 768)  0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 17, 17, 192)  258048      activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 17, 17, 192)  258048      activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 17, 17, 192)  576         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 17, 17, 192)  576         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 17, 17, 192)  576         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 17, 17, 192)  576         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 17, 17, 192)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 17, 17, 192)  0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 17, 17, 192)  0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 17, 17, 192)  0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_61[0][0]              \n",
      "                                                                 activation_64[0][0]              \n",
      "                                                                 activation_69[0][0]              \n",
      "                                                                 activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 17, 17, 192)  576         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 17, 17, 192)  0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 17, 17, 192)  258048      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 17, 17, 192)  576         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 17, 17, 192)  0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 17, 17, 192)  258048      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 17, 17, 192)  576         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 17, 17, 192)  576         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 17, 17, 192)  0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 17, 17, 192)  0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 8, 8, 320)    552960      activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 8, 8, 192)    331776      activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 8, 8, 320)    960         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 8, 8, 192)    576         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 8, 8, 320)    0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 8, 8, 192)    0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 768)    0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 8, 8, 1280)   0           activation_72[0][0]              \n",
      "                                                                 activation_76[0][0]              \n",
      "                                                                 max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 8, 8, 448)    573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 8, 8, 448)    1344        conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 8, 8, 448)    0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 8, 8, 384)    491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 8, 8, 384)    1548288     activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 8, 8, 384)    1152        conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 8, 8, 384)    1152        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 8, 8, 384)    0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 8, 8, 384)    0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, 8, 8, 1280)   0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 8, 8, 320)    409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 8, 8, 384)    1152        conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 8, 8, 384)    1152        conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 8, 8, 384)    1152        conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 8, 8, 384)    1152        conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 8, 8, 192)    245760      average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 8, 8, 320)    960         conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 8, 8, 384)    0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 8, 8, 384)    0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 8, 8, 384)    0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 8, 8, 384)    0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 8, 8, 192)    576         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 8, 8, 320)    0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 8, 8, 768)    0           activation_79[0][0]              \n",
      "                                                                 activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 8, 8, 768)    0           activation_83[0][0]              \n",
      "                                                                 activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 8, 8, 192)    0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 8, 8, 2048)   0           activation_77[0][0]              \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 8, 8, 448)    917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 8, 8, 448)    1344        conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 8, 8, 448)    0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 8, 8, 384)    786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 8, 8, 384)    1548288     activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 8, 8, 384)    1152        conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 8, 8, 384)    1152        conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 8, 8, 384)    0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 8, 8, 384)    0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_9 (AveragePoo (None, 8, 8, 2048)   0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 8, 8, 320)    655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 8, 8, 384)    1152        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 8, 8, 384)    1152        conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 8, 8, 384)    1152        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 8, 8, 384)    1152        conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 8, 8, 192)    393216      average_pooling2d_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 8, 8, 320)    960         conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 8, 8, 384)    0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 8, 8, 384)    0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 8, 8, 384)    0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 8, 8, 384)    0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 8, 8, 192)    576         conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 8, 8, 320)    0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 8, 8, 768)    0           activation_88[0][0]              \n",
      "                                                                 activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 8, 8, 768)    0           activation_92[0][0]              \n",
      "                                                                 activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 8, 8, 192)    0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 8, 8, 2048)   0           activation_86[0][0]              \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_2[0][0]              \n",
      "                                                                 activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 33)           2140193     mixed10[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 23,942,977\n",
      "Trainable params: 23,904,449\n",
      "Non-trainable params: 38,528\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Stack top_model on top\n",
    "model = Model(inputs=base_model.input, outputs=top_model(base_model.output))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Establish baseline accuracy with untrained model (labeled validation images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 418 images belonging to 33 classes.\n"
     ]
    }
   ],
   "source": [
    "valid_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "valid_generator = valid_datagen.flow_from_directory(directory=valid_folder,\n",
    "                                                    target_size=(INPUT_SIZE,INPUT_SIZE),\n",
    "                                                    batch_size=BATCH_SIZE,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 4s 165ms/step\n"
     ]
    }
   ],
   "source": [
    "generator = valid_generator\n",
    "ypred = model.predict_generator(generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = generator.filenames\n",
    "df = pd.DataFrame.from_dict(generator.class_indices, orient='index')\n",
    "class_list = df.sort_values(by=0).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of 418 images tested, top1 accuracy: 3.11% (405 wrong) and top3 accuracy: 11.24% (371 wrong)\n"
     ]
    }
   ],
   "source": [
    "true_classes = generator.classes\n",
    "pred1_classes = np.argsort(ypred, axis=1)[:,-1] # or np.argmax(ypred, axis=1)\n",
    "pred2_classes = np.argsort(ypred, axis=1)[:,-2]\n",
    "pred3_classes = np.argsort(ypred, axis=1)[:,-3]\n",
    "top1_error_idx = (true_classes != pred1_classes)\n",
    "top3_error_idx = (true_classes != pred1_classes) & (true_classes != pred2_classes) & (true_classes != pred3_classes)\n",
    "\n",
    "print (\"Of {} images tested, top1 accuracy: {:.2f}% ({} wrong) and top3 accuracy: {:.2f}% ({} wrong)\".format(\n",
    "        len(img_list),\n",
    "        (1-np.sum(top1_error_idx)/len(img_list))*100, np.sum(top1_error_idx),\n",
    "        (1-np.sum(top3_error_idx)/len(img_list))*100, np.sum(top3_error_idx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: Running the cell below can take a long time because of the large number of errors with untrained model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "error_idx = top3_error_idx\n",
    "\n",
    "for img_path, cat, pred1, pred2, pred3 in zip(list(compress(img_list, error_idx)),\n",
    "                                              [class_list[int(b)] for b in true_classes[error_idx]],\n",
    "                                              [class_list[int(b)] for b in pred1_classes[error_idx]],\n",
    "                                              [class_list[int(b)] for b in pred2_classes[error_idx]],\n",
    "                                              [class_list[int(b)] for b in pred3_classes[error_idx]]):\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    img = image.load_img(join(valid_folder, img_path), target_size=(INPUT_SIZE,INPUT_SIZE))\n",
    "    img = image.img_to_array(img)\n",
    "    ax.imshow(img / 255.)\n",
    "    ax.text(10, 250, 'P1: %s' % pred1, color='w', backgroundcolor='r', alpha=0.8)\n",
    "    ax.text(10, 270, 'P2: %s' % pred2, color='w', backgroundcolor='r', alpha=0.8)\n",
    "    ax.text(10, 290, 'P3: %s' % pred3, color='w', backgroundcolor='r', alpha=0.8)\n",
    "    ax.text(10, 310, 'LABEL: %s' % cat, color='k', backgroundcolor='g', alpha=0.8)\n",
    "    ax.axis('off')\n",
    "    plt.show()       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train 'top model' with bottleneck features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/bckenstler/CLR for Cyclical Learning Rate\n",
    "class CyclicLR(Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or \n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "    \n",
    "    # Example\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    \n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```    \n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore \n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where \n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored \n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on \n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1725 samples, validate on 418 samples\n",
      "Epoch 1/50\n",
      "1725/1725 [==============================] - 2s 1ms/step - loss: 2.5017 - acc: 0.3699 - val_loss: 1.7899 - val_acc: 0.6220\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.62201, saving model to models/2018-06-22_top_model.h5\n",
      "Epoch 2/50\n",
      "1725/1725 [==============================] - 1s 811us/step - loss: 1.7869 - acc: 0.5594 - val_loss: 1.3611 - val_acc: 0.7416\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.62201 to 0.74163, saving model to models/2018-06-22_top_model.h5\n",
      "Epoch 3/50\n",
      "1725/1725 [==============================] - 1s 818us/step - loss: 1.4426 - acc: 0.6435 - val_loss: 1.0847 - val_acc: 0.7823\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.74163 to 0.78230, saving model to models/2018-06-22_top_model.h5\n",
      "Epoch 4/50\n",
      "1725/1725 [==============================] - 2s 882us/step - loss: 1.1865 - acc: 0.7142 - val_loss: 0.8924 - val_acc: 0.8110\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.78230 to 0.81100, saving model to models/2018-06-22_top_model.h5\n",
      "Epoch 5/50\n",
      "1725/1725 [==============================] - 1s 846us/step - loss: 0.9833 - acc: 0.7548 - val_loss: 0.7500 - val_acc: 0.8373\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.81100 to 0.83732, saving model to models/2018-06-22_top_model.h5\n",
      "Epoch 6/50\n",
      "1725/1725 [==============================] - 1s 827us/step - loss: 0.8412 - acc: 0.7890 - val_loss: 0.6489 - val_acc: 0.8445\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.83732 to 0.84450, saving model to models/2018-06-22_top_model.h5\n",
      "Epoch 7/50\n",
      "1725/1725 [==============================] - 1s 819us/step - loss: 0.7021 - acc: 0.8377 - val_loss: 0.5617 - val_acc: 0.8612\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.84450 to 0.86124, saving model to models/2018-06-22_top_model.h5\n",
      "Epoch 8/50\n",
      "1725/1725 [==============================] - 1s 862us/step - loss: 0.6072 - acc: 0.8470 - val_loss: 0.5004 - val_acc: 0.8708\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.86124 to 0.87081, saving model to models/2018-06-22_top_model.h5\n",
      "Epoch 9/50\n",
      "1725/1725 [==============================] - 1s 835us/step - loss: 0.5295 - acc: 0.8707 - val_loss: 0.4463 - val_acc: 0.8732\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.87081 to 0.87321, saving model to models/2018-06-22_top_model.h5\n",
      "Epoch 10/50\n",
      "1725/1725 [==============================] - 1s 823us/step - loss: 0.4435 - acc: 0.8945 - val_loss: 0.4143 - val_acc: 0.8828\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.87321 to 0.88278, saving model to models/2018-06-22_top_model.h5\n",
      "Epoch 11/50\n",
      "1725/1725 [==============================] - 1s 809us/step - loss: 0.4123 - acc: 0.9020 - val_loss: 0.3775 - val_acc: 0.8923\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.88278 to 0.89234, saving model to models/2018-06-22_top_model.h5\n",
      "Epoch 12/50\n",
      "1725/1725 [==============================] - 1s 813us/step - loss: 0.3751 - acc: 0.9043 - val_loss: 0.3546 - val_acc: 0.8947\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.89234 to 0.89474, saving model to models/2018-06-22_top_model.h5\n",
      "Epoch 13/50\n",
      "1725/1725 [==============================] - 1s 819us/step - loss: 0.3226 - acc: 0.9183 - val_loss: 0.3354 - val_acc: 0.9043\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.89474 to 0.90431, saving model to models/2018-06-22_top_model.h5\n",
      "Epoch 14/50\n",
      "1725/1725 [==============================] - 1s 819us/step - loss: 0.2846 - acc: 0.9252 - val_loss: 0.3214 - val_acc: 0.8995\n",
      "\n",
      "Epoch 00014: val_acc did not improve\n",
      "Epoch 15/50\n",
      "1725/1725 [==============================] - 1s 834us/step - loss: 0.2459 - acc: 0.9432 - val_loss: 0.3057 - val_acc: 0.9091\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.90431 to 0.90909, saving model to models/2018-06-22_top_model.h5\n",
      "Epoch 16/50\n",
      "1725/1725 [==============================] - 1s 808us/step - loss: 0.2190 - acc: 0.9525 - val_loss: 0.2891 - val_acc: 0.9115\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.90909 to 0.91148, saving model to models/2018-06-22_top_model.h5\n",
      "Epoch 17/50\n",
      "1725/1725 [==============================] - 1s 821us/step - loss: 0.1848 - acc: 0.9588 - val_loss: 0.2805 - val_acc: 0.9115\n",
      "\n",
      "Epoch 00017: val_acc did not improve\n",
      "Epoch 18/50\n",
      "1725/1725 [==============================] - 1s 811us/step - loss: 0.1953 - acc: 0.9548 - val_loss: 0.2714 - val_acc: 0.9187\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.91148 to 0.91866, saving model to models/2018-06-22_top_model.h5\n",
      "Epoch 19/50\n",
      "1725/1725 [==============================] - 1s 811us/step - loss: 0.1621 - acc: 0.9635 - val_loss: 0.2655 - val_acc: 0.9187\n",
      "\n",
      "Epoch 00019: val_acc did not improve\n",
      "Epoch 20/50\n",
      "1725/1725 [==============================] - 1s 810us/step - loss: 0.1437 - acc: 0.9681 - val_loss: 0.2549 - val_acc: 0.9211\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.91866 to 0.92105, saving model to models/2018-06-22_top_model.h5\n",
      "Epoch 21/50\n",
      "1725/1725 [==============================] - 1s 818us/step - loss: 0.1262 - acc: 0.9716 - val_loss: 0.2489 - val_acc: 0.9211\n",
      "\n",
      "Epoch 00021: val_acc did not improve\n",
      "Epoch 22/50\n",
      "1725/1725 [==============================] - 1s 825us/step - loss: 0.1288 - acc: 0.9710 - val_loss: 0.2453 - val_acc: 0.9306\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.92105 to 0.93062, saving model to models/2018-06-22_top_model.h5\n",
      "Epoch 23/50\n",
      "1725/1725 [==============================] - 1s 837us/step - loss: 0.1062 - acc: 0.9710 - val_loss: 0.2423 - val_acc: 0.9211\n",
      "\n",
      "Epoch 00023: val_acc did not improve\n",
      "Epoch 24/50\n",
      "1725/1725 [==============================] - 1s 853us/step - loss: 0.1061 - acc: 0.9780 - val_loss: 0.2441 - val_acc: 0.9258\n",
      "\n",
      "Epoch 00024: val_acc did not improve\n",
      "Epoch 25/50\n",
      "1725/1725 [==============================] - 1s 825us/step - loss: 0.1213 - acc: 0.9710 - val_loss: 0.2418 - val_acc: 0.9211\n",
      "\n",
      "Epoch 00025: val_acc did not improve\n",
      "Epoch 26/50\n",
      "1725/1725 [==============================] - 1s 832us/step - loss: 0.0999 - acc: 0.9774 - val_loss: 0.2456 - val_acc: 0.9234\n",
      "\n",
      "Epoch 00026: val_acc did not improve\n",
      "Epoch 27/50\n",
      "1725/1725 [==============================] - 1s 841us/step - loss: 0.1039 - acc: 0.9716 - val_loss: 0.2452 - val_acc: 0.9258\n",
      "\n",
      "Epoch 00027: val_acc did not improve\n",
      "Epoch 28/50\n",
      "1725/1725 [==============================] - 1s 825us/step - loss: 0.0809 - acc: 0.9786 - val_loss: 0.2395 - val_acc: 0.9306\n",
      "\n",
      "Epoch 00028: val_acc did not improve\n",
      "Epoch 29/50\n",
      "1725/1725 [==============================] - 1s 828us/step - loss: 0.0900 - acc: 0.9768 - val_loss: 0.2358 - val_acc: 0.9258\n",
      "\n",
      "Epoch 00029: val_acc did not improve\n",
      "Epoch 30/50\n",
      "1725/1725 [==============================] - 1s 820us/step - loss: 0.0778 - acc: 0.9820 - val_loss: 0.2340 - val_acc: 0.9282\n",
      "\n",
      "Epoch 00030: val_acc did not improve\n",
      "Epoch 31/50\n",
      "1725/1725 [==============================] - 1s 823us/step - loss: 0.0854 - acc: 0.9814 - val_loss: 0.2377 - val_acc: 0.9234\n",
      "\n",
      "Epoch 00031: val_acc did not improve\n",
      "Epoch 32/50\n",
      "1725/1725 [==============================] - 1s 827us/step - loss: 0.0686 - acc: 0.9896 - val_loss: 0.2403 - val_acc: 0.9187\n",
      "\n",
      "Epoch 00032: val_acc did not improve\n",
      "Epoch 33/50\n",
      "1725/1725 [==============================] - 1s 830us/step - loss: 0.0785 - acc: 0.9843 - val_loss: 0.2412 - val_acc: 0.9258\n",
      "\n",
      "Epoch 00033: val_acc did not improve\n",
      "Epoch 34/50\n",
      "1725/1725 [==============================] - 1s 812us/step - loss: 0.0660 - acc: 0.9890 - val_loss: 0.2420 - val_acc: 0.9211\n",
      "\n",
      "Epoch 00034: val_acc did not improve\n",
      "Epoch 35/50\n",
      "1725/1725 [==============================] - 1s 826us/step - loss: 0.0773 - acc: 0.9820 - val_loss: 0.2435 - val_acc: 0.9187\n",
      "\n",
      "Epoch 00035: val_acc did not improve\n",
      "Epoch 00035: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3ef0080ba8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save weights of best training epoch: monitor either val_loss or val_acc\n",
    "STAMP = \"{}_top_model\".format(datetime.datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "top_model_weights_path = \"models/{}.h5\".format(STAMP)\n",
    "\n",
    "# Authors suggest setting step_size = (2-8) x (training iterations in epoch)\n",
    "step_size = 2000\n",
    "clr = CyclicLR(base_lr=0.008,\n",
    "               max_lr=0.03,\n",
    "               step_size=step_size,\n",
    "               mode='exp_range',\n",
    "               gamma=0.99994)\n",
    "\n",
    "callbacks_list = [\n",
    "    ModelCheckpoint(top_model_weights_path, monitor='val_acc', verbose=1, save_best_only=True),\n",
    "    EarlyStopping(monitor='val_loss', patience=5, verbose=1),\n",
    "    clr\n",
    "]\n",
    "\n",
    "top_model.fit(bottleneck_features_train, bottleneck_labels_train,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=top_model_epochs,\n",
    "              callbacks=callbacks_list,\n",
    "              validation_data=(bottleneck_features_valid, bottleneck_labels_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Retest with trained 'top model' (labeled validation images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 418 images belonging to 33 classes.\n"
     ]
    }
   ],
   "source": [
    "valid_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "valid_generator = valid_datagen.flow_from_directory(directory=valid_folder,\n",
    "                                                    target_size=(INPUT_SIZE,INPUT_SIZE),\n",
    "                                                    batch_size=BATCH_SIZE,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 3s 116ms/step\n"
     ]
    }
   ],
   "source": [
    "generator = valid_generator\n",
    "ypred = model.predict_generator(generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = generator.filenames\n",
    "df = pd.DataFrame.from_dict(generator.class_indices, orient='index')\n",
    "class_list = df.sort_values(by=0).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of 418 images tested, top1 accuracy: 91.87% (34 wrong) and top3 accuracy: 97.37% (11 wrong)\n"
     ]
    }
   ],
   "source": [
    "true_classes = generator.classes\n",
    "pred1_classes = np.argsort(ypred, axis=1)[:,-1] # or np.argmax(ypred, axis=1)\n",
    "pred2_classes = np.argsort(ypred, axis=1)[:,-2]\n",
    "pred3_classes = np.argsort(ypred, axis=1)[:,-3]\n",
    "top1_error_idx = (true_classes != pred1_classes)\n",
    "top3_error_idx = (true_classes != pred1_classes) & (true_classes != pred2_classes) & (true_classes != pred3_classes)\n",
    "\n",
    "print (\"Of {} images tested, top1 accuracy: {:.2f}% ({} wrong) and top3 accuracy: {:.2f}% ({} wrong)\".format(\n",
    "        len(img_list),\n",
    "        (1-np.sum(top1_error_idx)/len(img_list))*100, np.sum(top1_error_idx),\n",
    "        (1-np.sum(top3_error_idx)/len(img_list))*100, np.sum(top3_error_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "error_idx = top3_error_idx\n",
    "\n",
    "for img_path, cat, pred1, pred2, pred3 in zip(list(compress(img_list, error_idx)),\n",
    "                                              [class_list[int(b)] for b in true_classes[error_idx]],\n",
    "                                              [class_list[int(b)] for b in pred1_classes[error_idx]],\n",
    "                                              [class_list[int(b)] for b in pred2_classes[error_idx]],\n",
    "                                              [class_list[int(b)] for b in pred3_classes[error_idx]]):\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    img = image.load_img(join(valid_folder, img_path), target_size=(INPUT_SIZE,INPUT_SIZE))\n",
    "    img = image.img_to_array(img)\n",
    "    ax.imshow(img / 255.)\n",
    "    ax.text(10, 250, 'P1: %s' % pred1, color='w', backgroundcolor='r', alpha=0.8)\n",
    "    ax.text(10, 270, 'P2: %s' % pred2, color='w', backgroundcolor='r', alpha=0.8)\n",
    "    ax.text(10, 290, 'P3: %s' % pred3, color='w', backgroundcolor='r', alpha=0.8)\n",
    "    ax.text(10, 310, 'LABEL: %s' % cat, color='k', backgroundcolor='g', alpha=0.8)\n",
    "    ax.axis('off')\n",
    "    plt.show()       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Setup data to flow from directory with real-time augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1725 images belonging to 33 classes.\n",
      "Found 418 images belonging to 33 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1. / 255,\n",
    "                                   rotation_range=45,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.25,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "train_generator = train_datagen.flow_from_directory(directory=train_folder,\n",
    "                                                    target_size=(INPUT_SIZE,INPUT_SIZE),\n",
    "                                                    batch_size=BATCH_SIZE,\n",
    "                                                    class_mode='categorical')\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "valid_generator = valid_datagen.flow_from_directory(directory=valid_folder,\n",
    "                                                    target_size=(INPUT_SIZE,INPUT_SIZE),\n",
    "                                                    batch_size=BATCH_SIZE,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Fine-tune 'base + top models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "107/107 [==============================] - 43s 406ms/step - loss: 1.0672 - acc: 0.6935 - val_loss: 0.3050 - val_acc: 0.9014\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.90144, saving model to models/2018-06-22_complete_model.h5\n",
      "Epoch 2/50\n",
      "107/107 [==============================] - 42s 396ms/step - loss: 0.7377 - acc: 0.7915 - val_loss: 0.3109 - val_acc: 0.9062\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.90144 to 0.90625, saving model to models/2018-06-22_complete_model.h5\n",
      "Epoch 3/50\n",
      "107/107 [==============================] - 42s 393ms/step - loss: 0.5605 - acc: 0.8343 - val_loss: 0.3007 - val_acc: 0.9038\n",
      "\n",
      "Epoch 00003: val_acc did not improve\n",
      "Epoch 4/50\n",
      "107/107 [==============================] - 42s 397ms/step - loss: 0.4686 - acc: 0.8608 - val_loss: 0.2965 - val_acc: 0.9038\n",
      "\n",
      "Epoch 00004: val_acc did not improve\n",
      "Epoch 5/50\n",
      "107/107 [==============================] - 42s 394ms/step - loss: 0.4190 - acc: 0.8801 - val_loss: 0.2899 - val_acc: 0.9087\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.90625 to 0.90865, saving model to models/2018-06-22_complete_model.h5\n",
      "Epoch 6/50\n",
      "107/107 [==============================] - 42s 391ms/step - loss: 0.3809 - acc: 0.8847 - val_loss: 0.3111 - val_acc: 0.9062\n",
      "\n",
      "Epoch 00006: val_acc did not improve\n",
      "Epoch 7/50\n",
      "107/107 [==============================] - 41s 386ms/step - loss: 0.3584 - acc: 0.8912 - val_loss: 0.3115 - val_acc: 0.9038\n",
      "\n",
      "Epoch 00007: val_acc did not improve\n",
      "Epoch 8/50\n",
      " 51/107 [=============>................] - ETA: 19s - loss: 0.3260 - acc: 0.8971"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-f37523b17b5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m                            \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                            \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                            validation_steps=valid_samples // BATCH_SIZE)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2222\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2223\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2224\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2226\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#for i, layer in enumerate(model.layers):\n",
    "#    print ('Layer #: {}, Name: {}'.format(i, layer.name))\n",
    "\n",
    "for layer in model.layers[:249]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "    layer.trainable = True\n",
    "    \n",
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "model.compile(optimizer = SGD(lr=1e-4, momentum=0.9),\n",
    "              loss ='categorical_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "# save weights of best training epoch: monitor either val_loss or val_acc\n",
    "STAMP = \"{}_complete_model\".format(datetime.datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "complete_model_weights_path = \"models/{}.h5\".format(STAMP)\n",
    "\n",
    "# Authors suggest setting step_size = (2-8) x (training iterations in epoch)\n",
    "step_size = 200\n",
    "clr = CyclicLR(base_lr=8*1e-5,\n",
    "               max_lr=4*1e-4,\n",
    "               step_size=step_size,\n",
    "               mode='exp_range',\n",
    "               gamma=0.99994)\n",
    "\n",
    "callbacks_list = [\n",
    "    ModelCheckpoint(complete_model_weights_path, monitor='val_acc', verbose=1, save_best_only=True),\n",
    "    EarlyStopping(monitor='val_loss', patience=5, verbose=1),\n",
    "    clr\n",
    "]\n",
    "\n",
    "hist = model.fit_generator(train_generator,\n",
    "                           steps_per_epoch=train_samples // BATCH_SIZE,\n",
    "                           epochs=complete_model_epochs,\n",
    "                           callbacks=callbacks_list,\n",
    "                           validation_data=valid_generator,\n",
    "                           validation_steps=valid_samples // BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 (a). Retest with fine-tuned model (labeled validation images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = valid_generator\n",
    "ypred = model.predict_generator(generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = generator.filenames\n",
    "df = pd.DataFrame.from_dict(generator.class_indices, orient='index')\n",
    "class_list = df.sort_values(by=0).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_classes = generator.classes\n",
    "pred1_classes = np.argsort(ypred, axis=1)[:,-1] # or np.argmax(ypred, axis=1)\n",
    "pred2_classes = np.argsort(ypred, axis=1)[:,-2]\n",
    "pred3_classes = np.argsort(ypred, axis=1)[:,-3]\n",
    "top1_error_idx = (true_classes != pred1_classes)\n",
    "top3_error_idx = (true_classes != pred1_classes) & (true_classes != pred2_classes) & (true_classes != pred3_classes)\n",
    "\n",
    "print (\"Of {} images tested, top1 accuracy: {:.2f}% ({} wrong) and top3 accuracy: {:.2f}% ({} wrong)\".format(\n",
    "        len(img_list),\n",
    "        (1-np.sum(top1_error_idx)/len(img_list))*100, np.sum(top1_error_idx),\n",
    "        (1-np.sum(top3_error_idx)/len(img_list))*100, np.sum(top3_error_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "error_idx = top3_error_idx\n",
    "\n",
    "for img_path, cat, pred1, pred2, pred3 in zip(list(compress(img_list, error_idx)),\n",
    "                                              [class_list[int(b)] for b in true_classes[error_idx]],\n",
    "                                              [class_list[int(b)] for b in pred1_classes[error_idx]],\n",
    "                                              [class_list[int(b)] for b in pred2_classes[error_idx]],\n",
    "                                              [class_list[int(b)] for b in pred3_classes[error_idx]]):\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    img = image.load_img(join(valid_folder, img_path), target_size=(INPUT_SIZE,INPUT_SIZE))\n",
    "    img = image.img_to_array(img)\n",
    "    ax.imshow(img / 255.)\n",
    "    ax.text(10, 250, 'P1: %s' % pred1, color='w', backgroundcolor='r', alpha=0.8)\n",
    "    ax.text(10, 270, 'P2: %s' % pred2, color='w', backgroundcolor='r', alpha=0.8)\n",
    "    ax.text(10, 290, 'P3: %s' % pred3, color='w', backgroundcolor='r', alpha=0.8)\n",
    "    ax.text(10, 310, 'LABEL: %s' % cat, color='k', backgroundcolor='g', alpha=0.8)\n",
    "    ax.axis('off')\n",
    "    plt.show()       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 (b). Test with unlabeled test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs = [f for f in listdir(test_folder) if isfile(join(test_folder, f))]\n",
    "x_test = np.zeros((len(test_imgs), INPUT_SIZE, INPUT_SIZE, 3), dtype='float32')\n",
    "\n",
    "for i, img_file in tqdm_notebook(enumerate(test_imgs)):\n",
    "    img = image.load_img(join(test_folder, img_file), target_size=(INPUT_SIZE,INPUT_SIZE))\n",
    "    img = image.img_to_array(img)\n",
    "    x = preprocess_input(np.expand_dims(img.copy(), axis=0))\n",
    "    x_test[i] = x\n",
    "print('Test Images shape: {} size: {:,}'.format(x_test.shape, x_test.size))\n",
    "\n",
    "generator = valid_generator\n",
    "ypred = model.predict(x_test, batch_size=BATCH_SIZE, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = test_imgs\n",
    "df = pd.DataFrame.from_dict(generator.class_indices, orient='index')\n",
    "class_list = df.sort_values(by=0).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1_classes = np.argsort(ypred, axis=1)[:,-1] # or np.argmax(ypred, axis=1)\n",
    "pred2_classes = np.argsort(ypred, axis=1)[:,-2]\n",
    "pred3_classes = np.argsort(ypred, axis=1)[:,-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for img_path, pred1, pred2, pred3 in zip(img_list,\n",
    "                                         [class_list[int(b)] for b in pred1_classes],\n",
    "                                         [class_list[int(b)] for b in pred2_classes],\n",
    "                                         [class_list[int(b)] for b in pred3_classes]):\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    img = image.load_img(join(test_folder, img_path), target_size=(INPUT_SIZE,INPUT_SIZE))\n",
    "    img = image.img_to_array(img)\n",
    "    ax.imshow(img / 255.)\n",
    "    ax.text(10, 250, 'P1: %s' % pred1, color='w', backgroundcolor='r', alpha=0.8)\n",
    "    ax.text(10, 270, 'P2: %s' % pred2, color='w', backgroundcolor='r', alpha=0.8)\n",
    "    ax.text(10, 290, 'P3: %s' % pred3, color='w', backgroundcolor='r', alpha=0.8)\n",
    "    ax.axis('off')\n",
    "    plt.show()       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_p36",
   "language": "python",
   "name": "tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
